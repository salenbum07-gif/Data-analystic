#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–†–û–í–ï–†–ö–ò –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•
–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞
"""

import pandas as pd


# ============================================================================
# –ü–†–û–í–ï–†–ö–ê –°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê
# ============================================================================

def check_balanced_dataset(data_file='dataset_balanced.csv'):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print("=" * 80)
    print("üìä –ü–†–û–í–ï–†–ö–ê –°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê")
    print("=" * 80)
    
    df = pd.read_csv(data_file)
    print(f"\n‚úÖ –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df):,} —Å—Ç—Ä–æ–∫")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    category_counts = df['category'].value_counts()
    print("\nüìÇ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    for category, count in category_counts.items():
        print(f"  {category:<30} {count:>8,}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüìè –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {df['text'].str.len().min()}")
    print(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {df['text'].str.len().max()}")
    print(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {df['text'].str.len().mean():.1f}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω—ã–µ —Å–ª–æ–≤–∞
    profanity_words = ['–±–ª—è–¥—å', '—Å—É–∫–∞', '–ø–∏–∑–¥–∞', '—Ö—É–π', '–µ–±–∞—Ç—å', '–≥–æ–≤–Ω–æ', '–¥–µ—Ä—å–º–æ']
    profanity_count = 0
    print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω—ã—Ö —Å–ª–æ–≤:")
    for word in profanity_words:
        count = df[df['text'].str.lower().str.contains(word, na=False)].shape[0]
        if count > 0:
            profanity_count += count
            print(f"  –ù–∞–π–¥–µ–Ω–æ \"{word}\": {count} —Ä–∞–∑")
    
    print(f"\nüìä –í—Å–µ–≥–æ –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω—ã—Ö —Å–ª–æ–≤: {profanity_count}")
    quality = "–û–¢–õ–ò–ß–ù–û" if profanity_count == 0 else "–¢–†–ï–ë–£–ï–¢ –î–û–†–ê–ë–û–¢–ö–ò"
    print(f"‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ: {quality}")
    
    # –ü—Ä–∏–º–µ—Ä—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    print("\nüìù –ü—Ä–∏–º–µ—Ä—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    for category in category_counts.index[:3]:
        examples = df[df['category'] == category]['text'].head(2)
        print(f"\n  {category.upper()}:")
        for i, text in enumerate(examples, 1):
            print(f"    {i}. {text[:80]}...")
    
    return {
        'total_records': len(df),
        'categories': len(category_counts),
        'profanity_count': profanity_count,
        'quality': quality
    }


# ============================================================================
# –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –î–ê–¢–ê–°–ï–¢–ê
# ============================================================================

def final_check_dataset(data_file='dataset.csv'):
    """–§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print("\n" + "=" * 80)
    print("üîç –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –î–ê–¢–ê–°–ï–¢–ê")
    print("=" * 80)
    
    df = pd.read_csv(data_file)
    print(f"\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape[0]:,} —Å—Ç—Ä–æ–∫")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω—ã–µ —Å–ª–æ–≤–∞
    profanity_words = ['–±–ª—è–¥—å', '—Å—É–∫–∞', '–ø–∏–∑–¥–∞', '—Ö—É–π', '–µ–±–∞—Ç—å', '–≥–æ–≤–Ω–æ', '–¥–µ—Ä—å–º–æ', 
                      '–±–ª—è', '–ø–∏–∑–¥–µ—Ü', '—Ö—É–π–Ω—è']
    threat_words = ['—É–±—å—é', '–∑–∞—Ä–µ–∂—É', '–∑–∞—Å—Ç—Ä–µ–ª—é', '–ø–æ–≤–µ—à—É', '–∑–∞–¥—É—à—É', '–∏–∑–Ω–∞—Å–∏–ª—É—é', 
                    '–∏–∑–æ–±—å—é', '—É–≥—Ä–æ–∂–∞—é', '—É–Ω–∏—á—Ç–æ–∂—É']
    
    profanity_count = 0
    threat_count = 0
    
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω—ã—Ö —Å–ª–æ–≤:")
    for word in profanity_words:
        count = df[df['text'].str.lower().str.contains(word, na=False)].shape[0]
        if count > 0:
            profanity_count += count
            print(f"  –ù–∞–π–¥–µ–Ω–æ \"{word}\": {count} —Ä–∞–∑")
    
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≥—Ä–æ–∑:")
    for word in threat_words:
        count = df[df['text'].str.lower().str.contains(word, na=False)].shape[0]
        if count > 0:
            threat_count += count
            print(f"  –ù–∞–π–¥–µ–Ω–æ \"{word}\": {count} —Ä–∞–∑")
    
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–û–í–ï–†–ö–ò:")
    print(f"  –í—Å–µ–≥–æ –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω—ã—Ö —Å–ª–æ–≤: {profanity_count}")
    print(f"  –í—Å–µ–≥–æ —É–≥—Ä–æ–∑: {threat_count}")
    
    quality = "–û–¢–õ–ò–ß–ù–û" if profanity_count == 0 and threat_count == 0 else "–¢–†–ï–ë–£–ï–¢ –î–û–†–ê–ë–û–¢–ö–ò"
    print(f"  ‚úÖ –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {quality}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–æ–≤
    df['text_length'] = df['text'].str.len()
    print(f"\nüìè –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤:")
    print(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {df['text_length'].min()} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {df['text_length'].max()} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {df['text_length'].mean():.1f} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  –ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {df['text_length'].median():.1f} —Å–∏–º–≤–æ–ª–æ–≤")
    
    return {
        'total_records': len(df),
        'profanity_count': profanity_count,
        'threat_count': threat_count,
        'quality': quality
    }


# ============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def run_all_checks():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫"""
    print("üéØ –ü–†–û–í–ï–†–ö–ò –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•")
    print("=" * 80)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    try:
        check_balanced_dataset('dataset_balanced.csv')
    except FileNotFoundError:
        print("‚ö†Ô∏è –§–∞–π–ª dataset_balanced.csv –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    try:
        final_check_dataset('dataset.csv')
    except FileNotFoundError:
        print("‚ö†Ô∏è –§–∞–π–ª dataset.csv –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    print("\n" + "=" * 80)
    print("‚úÖ –í–°–ï –ü–†–û–í–ï–†–ö–ò –ó–ê–í–ï–†–®–ï–ù–´!")
    print("=" * 80)


if __name__ == "__main__":
    run_all_checks()

