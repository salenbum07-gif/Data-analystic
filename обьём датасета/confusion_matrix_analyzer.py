#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ê–ù–ê–õ–ò–ó CONFUSION MATRIX –î–õ–Ø DATA ANALYST
–î–µ—Ç–∞–ª—å–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –¥–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:
- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ confusion matrix –∏–∑ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
- –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ (Precision, Recall, F1-Score, Accuracy)
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ (heatmap)
- –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ (Markdown, JSON, CSV)
- –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π

–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    from confusion_matrix_analyzer import ConfusionMatrixAnalyzer
    
    analyzer = ConfusionMatrixAnalyzer()
    matrix = analyzer.build_confusion_matrix(y_true, y_pred, classes)
    analyzer.print_detailed_report()
    analyzer.save_report('report.md')
    analyzer.create_all_visualizations()
    
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - pandas
    - numpy
    - matplotlib (–¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π)
    - seaborn (–¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π)
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
from collections import defaultdict, Counter

# –ò–º–ø–æ—Ä—Ç —Å–∏—Å—Ç–µ–º—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ—à–∏–±–æ–∫
try:
    from error_categories import ErrorCategoryManager, create_default_manager
    HAS_ERROR_CATEGORIES = True
except ImportError:
    HAS_ERROR_CATEGORIES = False
    ErrorCategoryManager = None

# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
try:
    import matplotlib
    matplotlib.use('Agg')  # –ù–µ–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –±—ç–∫–µ–Ω–¥ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–µ–∑ –ø–æ–∫–∞–∑–∞
    import matplotlib.pyplot as plt
    import seaborn as sns
    HAS_VISUALIZATION = True
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    plt.rcParams['font.family'] = 'DejaVu Sans'
    sns.set_style("whitegrid")
    sns.set_palette("husl")
except ImportError:
    HAS_VISUALIZATION = False
    plt = None
    sns = None

class ConfusionMatrixAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ (Confusion Matrix)"""
    
    def __init__(self, error_category_manager=None):
        self.confusion_matrix = None
        self.classes = None
        self.results = {}
        self.figures_dir = 'confusion_matrix_figures'
        if not os.path.exists(self.figures_dir):
            os.makedirs(self.figures_dir)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –æ—à–∏–±–æ–∫
        if HAS_ERROR_CATEGORIES:
            if error_category_manager is None:
                self.category_manager = create_default_manager()
            else:
                self.category_manager = error_category_manager
        else:
            self.category_manager = None
        
    def build_confusion_matrix(self, y_true, y_pred, classes=None):
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ confusion matrix
        
        Args:
            y_true: —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
            y_pred: —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
            classes: —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            confusion_matrix: 2D –º–∞—Å—Å–∏–≤ (pandas DataFrame)
        """
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
        if classes is None:
            classes = sorted(set(y_true) | set(y_pred))
        
        self.classes = classes
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
        if len(y_true) != len(y_pred):
            raise ValueError(f"–î–ª–∏–Ω—ã –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: y_true={len(y_true)}, y_pred={len(y_pred)}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã
        n_classes = len(classes)
        matrix = np.zeros((n_classes, n_classes), dtype=int)
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã
        class_to_idx = {cls: idx for idx, cls in enumerate(classes)}
        
        for true_label, pred_label in zip(y_true, y_pred):
            true_idx = class_to_idx.get(true_label, -1)
            pred_idx = class_to_idx.get(pred_label, -1)
            if true_idx >= 0 and pred_idx >= 0:
                matrix[true_idx][pred_idx] += 1
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame
        self.confusion_matrix = pd.DataFrame(
            matrix,
            index=classes,
            columns=classes
        )
        
        return self.confusion_matrix
    
    def calculate_metrics_from_matrix(self):
        """
        –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∏–∑ confusion matrix
        
        Returns:
            dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        """
        if self.confusion_matrix is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å confusion matrix")
        
        metrics = {}
        
        for i, true_class in enumerate(self.classes):
            tp = self.confusion_matrix.loc[true_class, true_class]  # True Positives
            
            # False Positives: –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞, –∫—Ä–æ–º–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö
            fp = self.confusion_matrix.loc[:, true_class].sum() - tp
            
            # False Negatives: –≤—Å–µ –∏—Å—Ç–∏–Ω–Ω—ã–µ —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞, –∫—Ä–æ–º–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö
            fn = self.confusion_matrix.loc[true_class, :].sum() - tp
            
            # True Negatives: –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ
            tn = self.confusion_matrix.values.sum() - tp - fp - fn
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
            accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
            
            metrics[true_class] = {
                'tp': int(tp),
                'fp': int(fp),
                'fn': int(fn),
                'tn': int(tn),
                'precision': round(precision, 4),
                'recall': round(recall, 4),
                'f1': round(f1, 4),
                'accuracy': round(accuracy, 4),
                'support': int(tp + fn)
            }
        
        return metrics
    
    def get_normalized_matrix(self):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
        
        Returns:
            normalized_matrix: DataFrame —Å –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏
        """
        if self.confusion_matrix is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å confusion matrix")
        
        normalized = self.confusion_matrix.copy()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å—Ç—Ä–æ–∫–∞–º (–∏—Å—Ç–∏–Ω–Ω—ã–º –∫–ª–∞—Å—Å–∞–º)
        row_sums = normalized.sum(axis=1)
        for cls in self.classes:
            if row_sums[cls] > 0:
                normalized.loc[cls] = (normalized.loc[cls] / row_sums[cls] * 100).round(2)
        
        return normalized
    
    def find_common_mistakes(self, top_n=10):
        """
        –ü–æ–∏—Å–∫ —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        
        Args:
            top_n: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-–æ—à–∏–±–æ–∫
        
        Returns:
            list –∫–æ—Ä—Ç–µ–∂–µ–π (true_class, pred_class, count)
        """
        if self.confusion_matrix is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å confusion matrix")
        
        mistakes = []
        
        for true_cls in self.classes:
            for pred_cls in self.classes:
                if true_cls != pred_cls:
                    count = self.confusion_matrix.loc[true_cls, pred_cls]
                    if count > 0:
                        mistakes.append((true_cls, pred_cls, int(count)))
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ—à–∏–±–æ–∫
        mistakes.sort(key=lambda x: x[2], reverse=True)
        
        return mistakes[:top_n]
    
    def interpret_errors(self):
        """
        –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        
        Returns:
            dict —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π
        """
        if self.confusion_matrix is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å confusion matrix")
        
        metrics = self.calculate_metrics_from_matrix()
        mistakes = self.find_common_mistakes(top_n=50)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        
        interpretation = {
            'problematic_classes': [],
            'symmetric_errors': [],
            'dominant_confusions': [],
            'low_performance_classes': [],
            'recommendations': [],
            'error_patterns': [],
            'class_stability': [],
            'confusion_clusters': [],
            'detailed_analysis': {}
        }
        
        # 1. –ö–ª–∞—Å—Å—ã —Å –Ω–∏–∑–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é
        for cls, m in metrics.items():
            if m['f1'] < 0.5 or m['precision'] < 0.5 or m['recall'] < 0.5:
                interpretation['low_performance_classes'].append({
                    'class': cls,
                    'precision': m['precision'],
                    'recall': m['recall'],
                    'f1': m['f1'],
                    'issues': []
                })
                
                if m['precision'] < 0.5:
                    interpretation['low_performance_classes'][-1]['issues'].append(
                        f"–ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å ({m['precision']:.2%}) - –º–Ω–æ–≥–æ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π"
                    )
                if m['recall'] < 0.5:
                    interpretation['low_performance_classes'][-1]['issues'].append(
                        f"–ù–∏–∑–∫–∞—è –ø–æ–ª–Ω–æ—Ç–∞ ({m['recall']:.2%}) - –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤"
                    )
        
        # 2. –°–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ (–∫–ª–∞—Å—Å—ã –ø—É—Ç–∞—é—Ç—Å—è –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º)
        mistake_dict = {(t, p): c for t, p, c in mistakes}
        for true_cls, pred_cls, count in mistakes:
            reverse_count = mistake_dict.get((pred_cls, true_cls), 0)
            if reverse_count > 0:
                # –≠—Ç–æ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞
                if not any(s['class1'] == true_cls and s['class2'] == pred_cls 
                          for s in interpretation['symmetric_errors']):
                    interpretation['symmetric_errors'].append({
                        'class1': true_cls,
                        'class2': pred_cls,
                        'count1_to_2': count,
                        'count2_to_1': reverse_count,
                        'total_mistakes': count + reverse_count
                    })
        
        # 3. –î–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ –æ—à–∏–±–∫–∏ (–±–æ–ª–µ–µ 20% –æ—Ç –∫–ª–∞—Å—Å–∞)
        for true_cls in self.classes:
            total_true = self.confusion_matrix.loc[true_cls, :].sum()
            for pred_cls in self.classes:
                if true_cls != pred_cls:
                    count = self.confusion_matrix.loc[true_cls, pred_cls]
                    percentage = (count / total_true * 100) if total_true > 0 else 0
                    if percentage >= 20:  # –ë–æ–ª–µ–µ 20% –æ—à–∏–±–æ–∫ –≤ —ç—Ç–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏
                        interpretation['dominant_confusions'].append({
                            'true_class': true_cls,
                            'predicted_class': pred_cls,
                            'count': int(count),
                            'percentage': round(percentage, 2),
                            'severity': '–∫—Ä–∏—Ç–∏—á–Ω–æ' if percentage >= 50 else '–≤—ã—Å–æ–∫–∞—è' if percentage >= 30 else '—Å—Ä–µ–¥–Ω—è—è'
                        })
        
        # 4. –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–ª–∞—Å—Å—ã (–º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ –≤–æ –≤—Å–µ—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö)
        for cls in self.classes:
            total_class = self.confusion_matrix.loc[cls, :].sum()
            correct = self.confusion_matrix.loc[cls, cls]
            error_rate = ((total_class - correct) / total_class * 100) if total_class > 0 else 0
            
            if error_rate >= 50:  # –ë–æ–ª–µ–µ 50% –æ—à–∏–±–æ–∫
                interpretation['problematic_classes'].append({
                    'class': cls,
                    'error_rate': round(error_rate, 2),
                    'correct': int(correct),
                    'total': int(total_class),
                    'main_confusions': []
                })
                
                # –ù–∞—Ö–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ—à–∏–±–æ–∫
                for pred_cls in self.classes:
                    if pred_cls != cls:
                        count = self.confusion_matrix.loc[cls, pred_cls]
                        if count > 0:
                            interpretation['problematic_classes'][-1]['main_confusions'].append({
                                'confused_with': pred_cls,
                                'count': int(count),
                                'percentage': round((count / total_class * 100), 2)
                            })
                
                # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É
                interpretation['problematic_classes'][-1]['main_confusions'].sort(
                    key=lambda x: x['count'], reverse=True
                )
                interpretation['problematic_classes'][-1]['main_confusions'] = \
                    interpretation['problematic_classes'][-1]['main_confusions'][:3]
        
        # 5. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–º –æ—à–∏–±–∫–∞–º
        if interpretation['symmetric_errors']:
            recommendations.append({
                'type': '—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ_–æ—à–∏–±–∫–∏',
                'priority': '–≤—ã—Å–æ–∫–∏–π',
                'description': f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(interpretation['symmetric_errors'])} –ø–∞—Ä –∫–ª–∞—Å—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø—É—Ç–∞—é—Ç—Å—è –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º",
                'action': '–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ —Ä–∞–∑–ª–∏—á–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ–∂–¥—É —ç—Ç–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏'
            })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–º –æ—à–∏–±–∫–∞–º
        critical_confusions = [c for c in interpretation['dominant_confusions'] if c['severity'] == '–∫—Ä–∏—Ç–∏—á–Ω–æ']
        if critical_confusions:
            recommendations.append({
                'type': '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ_–ø—É—Ç–∞–Ω–∏—Ü—ã',
                'priority': '–∫—Ä–∏—Ç–∏—á–Ω—ã–π',
                'description': f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(critical_confusions)} –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø—É—Ç–∞–Ω–∏—Ü (>50% –æ—à–∏–±–æ–∫)",
                'action': '–¢—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–º–æ—Ç—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤'
            })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–º –∫–ª–∞—Å—Å–∞–º
        if interpretation['low_performance_classes']:
            low_precision = [c for c in interpretation['low_performance_classes'] if c['precision'] < 0.5]
            low_recall = [c for c in interpretation['low_performance_classes'] if c['recall'] < 0.5]
            
            if low_precision:
                recommendations.append({
                    'type': '–Ω–∏–∑–∫–∞—è_—Ç–æ—á–Ω–æ—Å—Ç—å',
                    'priority': '–≤—ã—Å–æ–∫–∏–π',
                    'description': f"{len(low_precision)} –∫–ª–∞—Å—Å–æ–≤ –∏–º–µ—é—Ç –Ω–∏–∑–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å (<50%)",
                    'action': '–£–º–µ–Ω—å—à–∏—Ç—å –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤'
                })
            
            if low_recall:
                recommendations.append({
                    'type': '–Ω–∏–∑–∫–∞—è_–ø–æ–ª–Ω–æ—Ç–∞',
                    'priority': '–≤—ã—Å–æ–∫–∏–π',
                    'description': f"{len(low_recall)} –∫–ª–∞—Å—Å–æ–≤ –∏–º–µ—é—Ç –Ω–∏–∑–∫—É—é –ø–æ–ª–Ω–æ—Ç—É (<50%)",
                    'action': '–°–Ω–∏–∑–∏—Ç—å –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å –≤–µ—Å —ç—Ç–∏—Ö –∫–ª–∞—Å—Å–æ–≤'
                })
        
        interpretation['recommendations'] = recommendations
        
        # 6. –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö –ü–û –ö–ê–ñ–î–û–ú–£ –ö–õ–ê–°–°–£
        interpretation['detailed_analysis'] = self._detailed_class_analysis(metrics)
        
        # 7. –ê–ù–ê–õ–ò–ó –ü–ê–¢–¢–ï–†–ù–û–í –û–®–ò–ë–û–ö
        interpretation['error_patterns'] = self._analyze_error_patterns()
        
        # 8. –ê–ù–ê–õ–ò–ó –°–¢–ê–ë–ò–õ–¨–ù–û–°–¢–ò –ö–õ–ê–°–°–û–í
        interpretation['class_stability'] = self._analyze_class_stability(metrics)
        
        # 9. –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –ü–£–¢–ê–ù–ò–¶
        interpretation['confusion_clusters'] = self._find_confusion_clusters()
        
        # 10. –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö
        interpretation['error_statistics'] = self._calculate_error_statistics()
        
        # 11. –ö–ê–¢–ï–ì–û–†–ò–ó–ê–¶–ò–Ø –û–®–ò–ë–û–ö –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú
        if self.category_manager:
            interpretation['error_categorization'] = self._categorize_errors(metrics, interpretation)
        
        return interpretation
    
    def _categorize_errors(self, metrics, interpretation):
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –æ—à–∏–±–æ–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        categorization = {
            'by_category': {},  # –û—à–∏–±–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            'by_class': {},  # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
            'category_summary': {}  # –°–≤–æ–¥–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        }
        
        detailed = interpretation.get('detailed_analysis', {})
        patterns = interpretation.get('error_patterns', {})
        stability = interpretation.get('class_stability', [])
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        stability_dict = {s['class']: s for s in stability}
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –∫–ª–∞—Å—Å–∞–º
        for cls in self.classes:
            class_metrics = metrics.get(cls, {})
            class_analysis = detailed.get(cls, {})
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö
            class_analysis['stability_score'] = stability_dict.get(cls, {}).get('stability_score', 1.0)
            class_analysis['unique_error_classes'] = stability_dict.get(cls, {}).get('unique_error_classes', 0)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
            for ce in patterns.get('concentrated_errors', []):
                if ce['class'] == cls:
                    class_analysis['concentration'] = ce['concentration']
                    break
            
            if self.category_manager:
                categories = self.category_manager.categorize_error(
                    cls, class_metrics, class_analysis
                )
                categorization['by_class'][cls] = categories
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                for cat_name in categories:
                    if cat_name not in categorization['by_category']:
                        categorization['by_category'][cat_name] = []
                    categorization['by_category'][cat_name].append({
                        'class': cls,
                        'metrics': class_metrics,
                        'analysis': class_analysis
                    })
        
        # –°–≤–æ–¥–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        if self.category_manager:
            for cat_name, category in self.category_manager.categories.items():
                classes_in_category = categorization['by_category'].get(cat_name, [])
                categorization['category_summary'][cat_name] = {
                    'name': cat_name,
                    'description': category.description,
                    'severity': category.severity.value,
                    'classes_count': len(classes_in_category),
                    'classes': [item['class'] for item in classes_in_category],
                    'recommendations': category.recommendations
                }
        
        return categorization
    
    def _detailed_class_analysis(self, metrics):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞"""
        detailed = {}
        
        for cls in self.classes:
            m = metrics[cls]
            total_class = self.confusion_matrix.loc[cls, :].sum()
            correct = self.confusion_matrix.loc[cls, cls]
            errors = total_class - correct
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫
            error_distribution = {}
            for pred_cls in self.classes:
                if pred_cls != cls:
                    count = self.confusion_matrix.loc[cls, pred_cls]
                    if count > 0:
                        error_distribution[pred_cls] = {
                            'count': int(count),
                            'percentage': round((count / total_class * 100), 2),
                            'percentage_of_errors': round((count / errors * 100), 2) if errors > 0 else 0
                        }
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø—Ä–æ–±–ª–µ–º
            issues = []
            if m['precision'] < 0.3:
                issues.append('–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏_–Ω–∏–∑–∫–∞—è_—Ç–æ—á–Ω–æ—Å—Ç—å')
            elif m['precision'] < 0.5:
                issues.append('–Ω–∏–∑–∫–∞—è_—Ç–æ—á–Ω–æ—Å—Ç—å')
            
            if m['recall'] < 0.3:
                issues.append('–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏_–Ω–∏–∑–∫–∞—è_–ø–æ–ª–Ω–æ—Ç–∞')
            elif m['recall'] < 0.5:
                issues.append('–Ω–∏–∑–∫–∞—è_–ø–æ–ª–Ω–æ—Ç–∞')
            
            if m['f1'] < 0.3:
                issues.append('–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏_–Ω–∏–∑–∫–∏–π_f1')
            elif m['f1'] < 0.5:
                issues.append('–Ω–∏–∑–∫–∏–π_f1')
            
            # –ê–Ω–∞–ª–∏–∑ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –æ—à–∏–±–æ–∫
            most_confused_with = sorted(
                error_distribution.items(),
                key=lambda x: x[1]['count'],
                reverse=True
            )[:5]
            
            detailed[cls] = {
                'metrics': m,
                'total_samples': int(total_class),
                'correct_predictions': int(correct),
                'error_count': int(errors),
                'error_rate': round((errors / total_class * 100), 2) if total_class > 0 else 0,
                'error_distribution': error_distribution,
                'most_confused_with': [
                    {'class': k, 'count': v['count'], 'percentage': v['percentage']} 
                    for k, v in most_confused_with
                ],
                'issues': issues,
                'severity': self._calculate_severity(m, errors, total_class),
                'interpretation': self._generate_class_interpretation(cls, m, error_distribution, most_confused_with)
            }
        
        return detailed
    
    def _calculate_severity(self, metrics, errors, total):
        """–†–∞—Å—á–µ—Ç —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ –ø—Ä–æ–±–ª–µ–º –∫–ª–∞—Å—Å–∞"""
        error_rate = (errors / total * 100) if total > 0 else 0
        
        if metrics['f1'] < 0.3 or error_rate > 70:
            return '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è'
        elif metrics['f1'] < 0.5 or error_rate > 50:
            return '–≤—ã—Å–æ–∫–∞—è'
        elif metrics['f1'] < 0.7 or error_rate > 30:
            return '—Å—Ä–µ–¥–Ω—è—è'
        else:
            return '–Ω–∏–∑–∫–∞—è'
    
    def _generate_class_interpretation(self, cls, metrics, error_dist, most_confused):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –¥–ª—è –∫–ª–∞—Å—Å–∞"""
        interpretations = []
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–æ—á–Ω–æ—Å—Ç–∏
        if metrics['precision'] < 0.5:
            interpretations.append(
                f"–ö–ª–∞—Å—Å '{cls}' –∏–º–µ–µ—Ç –Ω–∏–∑–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å ({metrics['precision']:.1%}). "
                f"–ú–æ–¥–µ–ª—å —á–∞—Å—Ç–æ –æ—à–∏–±–æ—á–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –¥—Ä—É–≥–∏–µ –∫–ª–∞—Å—Å—ã –∫–∞–∫ '{cls}'. "
                f"–≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —ç—Ç–æ—Ç –∫–ª–∞—Å—Å."
            )
        elif metrics['precision'] > 0.9:
            interpretations.append(
                f"–ö–ª–∞—Å—Å '{cls}' –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å ({metrics['precision']:.1%}). "
                f"–ö–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —ç—Ç–æ—Ç –∫–ª–∞—Å—Å, –æ–Ω–∞ –æ–±—ã—á–Ω–æ –ø—Ä–∞–≤–∞."
            )
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ—Ç—ã
        if metrics['recall'] < 0.5:
            interpretations.append(
                f"–ö–ª–∞—Å—Å '{cls}' –∏–º–µ–µ—Ç –Ω–∏–∑–∫—É—é –ø–æ–ª–Ω–æ—Ç—É ({metrics['recall']:.1%}). "
                f"–ú–æ–¥–µ–ª—å –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –º–Ω–æ–≥–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞. "
                f"–≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ '{cls}'."
            )
        elif metrics['recall'] > 0.9:
            interpretations.append(
                f"–ö–ª–∞—Å—Å '{cls}' –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫—É—é –ø–æ–ª–Ω–æ—Ç—É ({metrics['recall']:.1%}). "
                f"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞."
            )
        
        # –ê–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—É—Ç–∞–Ω–∏—Ü
        if most_confused:
            top_confusion = most_confused[0]
            interpretations.append(
                f"–û—Å–Ω–æ–≤–Ω–∞—è –ø—É—Ç–∞–Ω–∏—Ü–∞: '{cls}' —á–∞—â–µ –≤—Å–µ–≥–æ –æ—à–∏–±–æ—á–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ "
                f"'{top_confusion[0]}' ({top_confusion[1]['count']} —Å–ª—É—á–∞–µ–≤, "
                f"{top_confusion[1]['percentage']:.1f}% –æ—Ç –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Å–∞). "
                f"–≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å –∏–ª–∏ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∏ –º–µ–∂–¥—É —ç—Ç–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏."
            )
        
        # –ê–Ω–∞–ª–∏–∑ F1
        if metrics['f1'] < 0.5:
            interpretations.append(
                f"–û–±—â–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ '{cls}' –Ω–∏–∑–∫–∞—è (F1={metrics['f1']:.1%}). "
                f"–¢—Ä–µ–±—É–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏, —Ç–∞–∫ –∏ –ø–æ–ª–Ω–æ—Ç—ã."
            )
        
        return " ".join(interpretations) if interpretations else f"–ö–ª–∞—Å—Å '{cls}' –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∏–µ–º–ª–µ–º—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å."
    
    def _analyze_error_patterns(self):
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—à–∏–±–æ–∫"""
        patterns = {
            'one_way_confusions': [],  # –û–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ –ø—É—Ç–∞–Ω–∏—Ü—ã
            'bidirectional_confusions': [],  # –î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ –ø—É—Ç–∞–Ω–∏—Ü—ã
            'scattered_errors': [],  # –†–∞–∑–±—Ä–æ—Å–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
            'concentrated_errors': []  # –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
        }
        
        for true_cls in self.classes:
            total_true = self.confusion_matrix.loc[true_cls, :].sum()
            correct = self.confusion_matrix.loc[true_cls, true_cls]
            errors = total_true - correct
            
            if errors == 0:
                continue
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫
            error_counts = []
            for pred_cls in self.classes:
                if pred_cls != true_cls:
                    count = self.confusion_matrix.loc[true_cls, pred_cls]
                    if count > 0:
                        error_counts.append((pred_cls, count, (count / errors * 100)))
            
            error_counts.sort(key=lambda x: x[1], reverse=True)
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            if len(error_counts) == 0:
                continue
            
            top_error_pct = error_counts[0][2] if error_counts else 0
            
            # –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ (>60% –≤ –æ–¥–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏)
            if top_error_pct > 60:
                patterns['concentrated_errors'].append({
                    'class': true_cls,
                    'main_confusion': error_counts[0][0],
                    'concentration': round(top_error_pct, 1),
                    'total_errors': int(errors)
                })
            # –†–∞–∑–±—Ä–æ—Å–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ (<30% –≤ –ª—é–±–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏)
            elif top_error_pct < 30:
                patterns['scattered_errors'].append({
                    'class': true_cls,
                    'error_distribution': len(error_counts),
                    'total_errors': int(errors),
                    'interpretation': '–û—à–∏–±–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É –º–Ω–æ–≥–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –æ–±—â—É—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞'
                })
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –ø—É—Ç–∞–Ω–∏—Ü
            for pred_cls, count, pct in error_counts[:3]:
                reverse_count = self.confusion_matrix.loc[pred_cls, true_cls]
                if reverse_count > 0:
                    reverse_pct = (reverse_count / self.confusion_matrix.loc[pred_cls, :].sum() * 100) if self.confusion_matrix.loc[pred_cls, :].sum() > 0 else 0
                    if not any(b['class1'] == true_cls and b['class2'] == pred_cls for b in patterns['bidirectional_confusions']):
                        patterns['bidirectional_confusions'].append({
                            'class1': true_cls,
                            'class2': pred_cls,
                            'count1_to_2': int(count),
                            'count2_to_1': int(reverse_count),
                            'pct1_to_2': round(pct, 1),
                            'pct2_to_1': round(reverse_pct, 1),
                            'interpretation': f"–ö–ª–∞—Å—Å—ã '{true_cls}' –∏ '{pred_cls}' –ø—É—Ç–∞—é—Ç—Å—è –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º –≤ –æ–±–æ–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å"
                        })
        
        return patterns
    
    def _analyze_class_stability(self, metrics):
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤"""
        stability = []
        
        for cls in self.classes:
            m = metrics[cls]
            total = self.confusion_matrix.loc[cls, :].sum()
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã–µ –æ—à–∏–±–æ—á–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è
            unique_errors = sum(1 for pred_cls in self.classes 
                              if pred_cls != cls and self.confusion_matrix.loc[cls, pred_cls] > 0)
            
            # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å = –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã –æ—à–∏–±–∫–∏
            error_entropy = 0
            errors = total - self.confusion_matrix.loc[cls, cls]
            if errors > 0:
                for pred_cls in self.classes:
                    if pred_cls != cls:
                        count = self.confusion_matrix.loc[cls, pred_cls]
                        if count > 0:
                            p = count / errors
                            error_entropy -= p * np.log2(p + 1e-10)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è (0 = –≤—Å–µ –æ—à–∏–±–∫–∏ –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Å–µ, 1 = —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
            max_entropy = np.log2(max(unique_errors, 1))
            normalized_entropy = (error_entropy / max_entropy) if max_entropy > 0 else 0
            
            stability.append({
                'class': cls,
                'stability_score': round(1 - normalized_entropy, 3),  # –í—ã—à–µ = —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ
                'unique_error_classes': unique_errors,
                'error_entropy': round(error_entropy, 3),
                'interpretation': self._interpret_stability(cls, normalized_entropy, unique_errors, m)
            })
        
        return sorted(stability, key=lambda x: x['stability_score'])
    
    def _interpret_stability(self, cls, entropy, unique_errors, metrics):
        """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∞"""
        if entropy < 0.3:
            return f"–ö–ª–∞—Å—Å '{cls}' –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ—à–∏–±–æ–∫ - –æ—à–∏–±–∫–∏ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É—é—Ç—Å—è –≤ 1-2 –∫–ª–∞—Å—Å–∞—Ö. –≠—Ç–æ —Ö–æ—Ä–æ—à–æ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è."
        elif entropy > 0.7:
            return f"–ö–ª–∞—Å—Å '{cls}' –∏–º–µ–µ—Ç –Ω–∏–∑–∫—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å - –æ—à–∏–±–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É {unique_errors} –∫–ª–∞—Å—Å–∞–º–∏. –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±—â–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏."
        else:
            return f"–ö–ª–∞—Å—Å '{cls}' –∏–º–µ–µ—Ç —Å—Ä–µ–¥–Ω—é—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ—à–∏–±–æ–∫."
    
    def _find_confusion_clusters(self):
        """–ü–æ–∏—Å–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø—É—Ç–∞–Ω–∏—Ü (–≥—Ä—É–ø–ø –∫–ª–∞—Å—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –ø—É—Ç–∞—é—Ç—Å—è)"""
        clusters = []
        processed_pairs = set()
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ –ø—É—Ç–∞–Ω–∏—Ü
        confusion_graph = {}
        for true_cls in self.classes:
            confusion_graph[true_cls] = []
            for pred_cls in self.classes:
                if true_cls != pred_cls:
                    count = self.confusion_matrix.loc[true_cls, pred_cls]
                    if count > 0:
                        total = self.confusion_matrix.loc[true_cls, :].sum()
                        pct = (count / total * 100) if total > 0 else 0
                        if pct > 10:  # –ë–æ–ª–µ–µ 10% –æ—à–∏–±–æ–∫
                            confusion_graph[true_cls].append((pred_cls, count, pct))
        
        # –ü–æ–∏—Å–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–∫–ª–∞—Å—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—É—Ç–∞—é—Ç—Å—è –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º)
        for cls1 in self.classes:
            for cls2 in self.classes:
                if cls1 >= cls2:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                    continue
                
                pair_key = tuple(sorted([cls1, cls2]))
                if pair_key in processed_pairs:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω—é—é –ø—É—Ç–∞–Ω–∏—Ü—É
                count1_to_2 = self.confusion_matrix.loc[cls1, cls2]
                count2_to_1 = self.confusion_matrix.loc[cls2, cls1]
                
                if count1_to_2 > 0 and count2_to_1 > 0:
                    total1 = self.confusion_matrix.loc[cls1, :].sum()
                    total2 = self.confusion_matrix.loc[cls2, :].sum()
                    pct1_to_2 = (count1_to_2 / total1 * 100) if total1 > 0 else 0
                    pct2_to_1 = (count2_to_1 / total2 * 100) if total2 > 0 else 0
                    
                    if pct1_to_2 > 5 or pct2_to_1 > 5:  # –ó–Ω–∞—á–∏–º–∞—è –ø—É—Ç–∞–Ω–∏—Ü–∞
                        clusters.append({
                            'classes': [cls1, cls2],
                            'count1_to_2': int(count1_to_2),
                            'count2_to_1': int(count2_to_1),
                            'pct1_to_2': round(pct1_to_2, 1),
                            'pct2_to_1': round(pct2_to_1, 1),
                            'total_confusions': int(count1_to_2 + count2_to_1),
                            'strength': '—Å–∏–ª—å–Ω–∞—è' if (pct1_to_2 > 20 and pct2_to_1 > 20) else '—Å—Ä–µ–¥–Ω—è—è' if (pct1_to_2 > 10 or pct2_to_1 > 10) else '—Å–ª–∞–±–∞—è'
                        })
                        processed_pairs.add(pair_key)
        
        return sorted(clusters, key=lambda x: x['total_confusions'], reverse=True)
    
    def _calculate_error_statistics(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫"""
        total_samples = self.confusion_matrix.values.sum()
        total_correct = sum(self.confusion_matrix.loc[cls, cls] for cls in self.classes)
        total_errors = total_samples - total_correct
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º
        error_by_class = {}
        for cls in self.classes:
            total_class = self.confusion_matrix.loc[cls, :].sum()
            correct = self.confusion_matrix.loc[cls, cls]
            errors = total_class - correct
            error_by_class[cls] = {
                'errors': int(errors),
                'error_rate': round((errors / total_class * 100), 2) if total_class > 0 else 0,
                'contribution_to_total_errors': round((errors / total_errors * 100), 2) if total_errors > 0 else 0
            }
        
        # –ö–ª–∞—Å—Å—ã —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –≤–∫–ª–∞–¥–æ–º –≤ –æ–±—â–∏–µ –æ—à–∏–±–∫–∏
        top_error_contributors = sorted(
            error_by_class.items(),
            key=lambda x: x[1]['contribution_to_total_errors'],
            reverse=True
        )[:5]
        
        return {
            'total_samples': int(total_samples),
            'total_correct': int(total_correct),
            'total_errors': int(total_errors),
            'overall_error_rate': round((total_errors / total_samples * 100), 2) if total_samples > 0 else 0,
            'error_by_class': error_by_class,
            'top_error_contributors': [
                {'class': k, 'errors': v['errors'], 'contribution_pct': v['contribution_to_total_errors']}
                for k, v in top_error_contributors
            ],
            'average_error_rate_per_class': round(
                np.mean([v['error_rate'] for v in error_by_class.values()]), 2
            ),
            'error_rate_std': round(
                np.std([v['error_rate'] for v in error_by_class.values()]), 2
            )
        }
    
    def print_matrix(self, normalized=False):
        """–í—ã–≤–æ–¥ –º–∞—Ç—Ä–∏—Ü—ã –≤ –∫–æ–Ω—Å–æ–ª—å"""
        if self.confusion_matrix is None:
            print("‚ùå –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞")
            return
        
        matrix_to_print = self.get_normalized_matrix() if normalized else self.confusion_matrix
        
        print("\n" + "=" * 100)
        title = "–ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–ê–Ø CONFUSION MATRIX (%)" if normalized else "CONFUSION MATRIX (–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)"
        print(f"üìä {title}")
        print("=" * 100)
        print("\n–°—Ç—Ä–æ–∫–∏ = –ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã | –ö–æ–ª–æ–Ω–∫–∏ = –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã")
        print("-" * 100)
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        true_pred = 'True\\Pred'
        header = f"{true_pred:<25}"
        for pred_cls in self.classes:
            # –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
            short_name = pred_cls[:15] + "..." if len(pred_cls) > 15 else pred_cls
            header += f"{short_name:>10}"
        print(header)
        print("-" * 100)
        
        # –°—Ç—Ä–æ–∫–∏ –º–∞—Ç—Ä–∏—Ü—ã
        for true_cls in self.classes:
            short_true = true_cls[:22] + "..." if len(true_cls) > 22 else true_cls
            row = f"{short_true:<25}"
            for pred_cls in self.classes:
                value = matrix_to_print.loc[true_cls, pred_cls]
                if normalized:
                    row += f"{value:>9.1f}%"
                else:
                    row += f"{int(value):>10}"
            print(row)
        
        print("=" * 100)
        
        # –î–∏–∞–≥–æ–Ω–∞–ª—å (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)
        diagonal_sum = sum(self.confusion_matrix.loc[cls, cls] for cls in self.classes)
        total = self.confusion_matrix.values.sum()
        accuracy = (diagonal_sum / total * 100) if total > 0 else 0
        
        print(f"\n‚úÖ –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {diagonal_sum:,} / {total:,} = {accuracy:.2f}%")
    
    def print_detailed_report(self):
        """–í—ã–≤–æ–¥ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        if self.confusion_matrix is None:
            print("‚ùå –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞")
            return
        
        metrics = self.calculate_metrics_from_matrix()
        
        print("\n" + "=" * 100)
        print("üìä –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û CONFUSION MATRIX")
        print("=" * 100)
        
        # –í—ã–≤–æ–¥ –º–∞—Ç—Ä–∏—Ü—ã
        self.print_matrix(normalized=False)
        
        print("\n" + "=" * 100)
        print("üìä –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–ê–Ø –ú–ê–¢–†–ò–¶–ê (–≤ % –æ—Ç –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞)")
        print("=" * 100)
        self.print_matrix(normalized=True)
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
        print("\n" + "=" * 100)
        print("üìã –ú–ï–¢–†–ò–ö–ò –ü–û –ö–õ–ê–°–°–ê–ú")
        print("=" * 100)
        print(f"{'–ö–ª–∞—Å—Å':<30} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}")
        print("-" * 100)
        
        for cls in self.classes:
            m = metrics[cls]
            print(f"{cls:<30} {m['precision']:>10.4f} {m['recall']:>10.4f} "
                  f"{m['f1']:>10.4f} {m['support']:>10}")
        
        # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏
        print("\n" + "=" * 100)
        print("‚ö†Ô∏è –¢–û–ü-10 –°–ê–ú–´–• –ß–ê–°–¢–´–• –û–®–ò–ë–û–ö")
        print("=" * 100)
        print(f"{'–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å':<30} {'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å':<30} {'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ':>10} {'% –æ—Ç –∫–ª–∞—Å—Å–∞':>12}")
        print("-" * 100)
        
        mistakes = self.find_common_mistakes(top_n=10)
        for true_cls, pred_cls, count in mistakes:
            total_true = self.confusion_matrix.loc[true_cls, :].sum()
            percentage = (count / total_true * 100) if total_true > 0 else 0
            print(f"{true_cls:<30} {pred_cls:<30} {count:>10} {percentage:>11.2f}%")
        
        # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        print("\n" + "=" * 100)
        print("üìä –û–ë–©–ò–ï –ú–ï–¢–†–ò–ö–ò")
        print("=" * 100)
        
        macro_precision = np.mean([m['precision'] for m in metrics.values()])
        macro_recall = np.mean([m['recall'] for m in metrics.values()])
        macro_f1 = np.mean([m['f1'] for m in metrics.values()])
        
        total_tp = sum(m['tp'] for m in metrics.values())
        total_fp = sum(m['fp'] for m in metrics.values())
        total_fn = sum(m['fn'] for m in metrics.values())
        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0
        
        print(f"Macro Precision:  {macro_precision:.4f}")
        print(f"Macro Recall:     {macro_recall:.4f}")
        print(f"Macro F1:         {macro_f1:.4f}")
        print(f"\nMicro Precision:  {micro_precision:.4f}")
        print(f"Micro Recall:     {micro_recall:.4f}")
        print(f"Micro F1:         {micro_f1:.4f}")
        
        total_samples = self.confusion_matrix.values.sum()
        correct = sum(self.confusion_matrix.loc[cls, cls] for cls in self.classes)
        accuracy = (correct / total_samples * 100) if total_samples > 0 else 0
        print(f"\n–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy): {correct:,} / {total_samples:,} = {accuracy:.2f}%")
        
        # –ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –æ—à–∏–±–æ–∫
        self.print_full_interpretation()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        interpretation = self.interpret_errors()
        
        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        if interpretation['problematic_classes']:
            print("\n‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ù–´–ï –ö–õ–ê–°–°–´ (–±–æ–ª–µ–µ 50% –æ—à–∏–±–æ–∫):")
            print("-" * 100)
            for pc in interpretation['problematic_classes']:
                print(f"\nüìå –ö–ª–∞—Å—Å: {pc['class']}")
                print(f"   –û—à–∏–±–æ–∫: {pc['error_rate']}% ({pc['total'] - pc['correct']:,} –∏–∑ {pc['total']:,})")
                print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö: {pc['correct']:,} ({100 - pc['error_rate']:.1f}%)")
                if pc['main_confusions']:
                    print(f"   –û—Å–Ω–æ–≤–Ω—ã–µ –ø—É—Ç–∞–Ω–∏—Ü—ã:")
                    for conf in pc['main_confusions']:
                        print(f"     ‚Üí {conf['confused_with']}: {conf['count']:,} ({conf['percentage']}%)")
        
        # –ö–ª–∞—Å—Å—ã —Å –Ω–∏–∑–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é
        if interpretation['low_performance_classes']:
            print("\nüìâ –ö–õ–ê–°–°–´ –° –ù–ò–ó–ö–û–ô –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨–Æ:")
            print("-" * 100)
            for lpc in interpretation['low_performance_classes']:
                print(f"\nüìå {lpc['class']}:")
                print(f"   Precision: {lpc['precision']:.2%} | Recall: {lpc['recall']:.2%} | F1: {lpc['f1']:.2%}")
                for issue in lpc['issues']:
                    print(f"   ‚ö†Ô∏è {issue}")
        
        # –°–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏
        if interpretation['symmetric_errors']:
            print("\nüîÑ –°–ò–ú–ú–ï–¢–†–ò–ß–ù–´–ï –û–®–ò–ë–ö–ò (–∫–ª–∞—Å—Å—ã –ø—É—Ç–∞—é—Ç—Å—è –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º):")
            print("-" * 100)
            for se in interpretation['symmetric_errors']:
                print(f"\nüìå {se['class1']} ‚Üî {se['class2']}:")
                print(f"   {se['class1']} ‚Üí {se['class2']}: {se['count1_to_2']:,} –æ—à–∏–±–æ–∫")
                print(f"   {se['class2']} ‚Üí {se['class1']}: {se['count2_to_1']:,} –æ—à–∏–±–æ–∫")
                print(f"   –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {se['total_mistakes']:,}")
                print(f"   üí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: –≠—Ç–∏ –∫–ª–∞—Å—Å—ã –∏–º–µ—é—Ç –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∏")
        
        # –î–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ –æ—à–∏–±–∫–∏
        if interpretation['dominant_confusions']:
            print("\nüìä –î–û–ú–ò–ù–ò–†–£–Æ–©–ò–ï –û–®–ò–ë–ö–ò (–±–æ–ª–µ–µ 20% –æ—Ç –∫–ª–∞—Å—Å–∞):")
            print("-" * 100)
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—Ç–µ–ø–µ–Ω–∏ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
            by_severity = {'–∫—Ä–∏—Ç–∏—á–Ω–æ': [], '–≤—ã—Å–æ–∫–∞—è': [], '—Å—Ä–µ–¥–Ω—è—è': []}
            for dc in interpretation['dominant_confusions']:
                by_severity[dc['severity']].append(dc)
            
            for severity, confusions in by_severity.items():
                if confusions:
                    severity_emoji = {'–∫—Ä–∏—Ç–∏—á–Ω–æ': 'üî¥', '–≤—ã—Å–æ–∫–∞—è': 'üü†', '—Å—Ä–µ–¥–Ω—è—è': 'üü°'}
                    print(f"\n{severity_emoji.get(severity, '‚Ä¢')} {severity.upper()}:")
                    for dc in sorted(confusions, key=lambda x: x['percentage'], reverse=True):
                        print(f"   {dc['true_class']} ‚Üí {dc['predicted_class']}: "
                              f"{dc['count']:,} ({dc['percentage']}% –æ—Ç –∫–ª–∞—Å—Å–∞)")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if interpretation['recommendations']:
            print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ:")
            print("-" * 100)
            for i, rec in enumerate(interpretation['recommendations'], 1):
                priority_emoji = {'–∫—Ä–∏—Ç–∏—á–Ω—ã–π': 'üî¥', '–≤—ã—Å–æ–∫–∏–π': 'üü†', '—Å—Ä–µ–¥–Ω–∏–π': 'üü°', '–Ω–∏–∑–∫–∏–π': 'üü¢'}
                print(f"\n{i}. {priority_emoji.get(rec['priority'], '‚Ä¢')} [{rec['priority'].upper()}] {rec['type'].replace('_', ' ').title()}")
                print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {rec['description']}")
                print(f"   –î–µ–π—Å—Ç–≤–∏–µ: {rec['action']}")
        
        print("\n" + "=" * 100)
    
    def print_full_interpretation(self):
        """–í—ã–≤–æ–¥ –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –æ—à–∏–±–æ–∫"""
        if self.confusion_matrix is None:
            print("‚ùå –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞")
            return
        
        interpretation = self.interpret_errors()
        
        print("\n" + "=" * 100)
        print("üîç –ü–û–õ–ù–ê–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –û–®–ò–ë–û–ö –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò")
        print("=" * 100)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫
        stats = interpretation['error_statistics']
        print("\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–®–ò–ë–û–ö")
        print("-" * 100)
        print(f"–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {stats['total_samples']:,}")
        print(f"–ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {stats['total_correct']:,} ({100 - stats['overall_error_rate']:.2f}%)")
        print(f"–û—à–∏–±–æ–∫: {stats['total_errors']:,} ({stats['overall_error_rate']:.2f}%)")
        print(f"–°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º: {stats['average_error_rate_per_class']:.2f}% (œÉ={stats['error_rate_std']:.2f}%)")
        
        if stats['top_error_contributors']:
            print("\nüî¥ –¢–û–ü-5 –ö–õ–ê–°–°–û–í –° –ù–ê–ò–ë–û–õ–¨–®–ò–ú –í–ö–õ–ê–î–û–ú –í –û–®–ò–ë–ö–ò:")
            for i, contrib in enumerate(stats['top_error_contributors'], 1):
                print(f"  {i}. {contrib['class']}: {contrib['errors']:,} –æ—à–∏–±–æ–∫ ({contrib['contribution_pct']:.1f}% –æ—Ç –≤—Å–µ—Ö –æ—à–∏–±–æ–∫)")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Å–∞–º
        print("\n" + "=" * 100)
        print("üìã –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–û –ö–ê–ñ–î–û–ú–£ –ö–õ–ê–°–°–£")
        print("=" * 100)
        
        detailed = interpretation['detailed_analysis']
        for cls in sorted(detailed.keys(), key=lambda x: detailed[x]['severity'] == '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è', reverse=True):
            analysis = detailed[cls]
            severity_emoji = {'–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è': 'üî¥', '–≤—ã—Å–æ–∫–∞—è': 'üü†', '—Å—Ä–µ–¥–Ω—è—è': 'üü°', '–Ω–∏–∑–∫–∞—è': 'üü¢'}
            emoji = severity_emoji.get(analysis['severity'], '‚Ä¢')
            
            print(f"\n{emoji} –ö–õ–ê–°–°: {cls}")
            print(f"   –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º: {analysis['severity'].upper()}")
            print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {analysis['total_samples']:,}")
            print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö: {analysis['correct_predictions']:,} ({100 - analysis['error_rate']:.1f}%)")
            print(f"   –û—à–∏–±–æ–∫: {analysis['error_count']:,} ({analysis['error_rate']:.1f}%)")
            print(f"   Precision: {analysis['metrics']['precision']:.3f} | Recall: {analysis['metrics']['recall']:.3f} | F1: {analysis['metrics']['f1']:.3f}")
            
            if analysis['issues']:
                print(f"   –ü—Ä–æ–±–ª–µ–º—ã: {', '.join(analysis['issues'])}")
            
            if analysis['most_confused_with']:
                print(f"   –û—Å–Ω–æ–≤–Ω—ã–µ –ø—É—Ç–∞–Ω–∏—Ü—ã:")
                for conf in analysis['most_confused_with'][:3]:
                    print(f"     ‚Üí {conf['class']}: {conf['count']:,} ({conf['percentage']:.1f}%)")
            
            print(f"   üìù –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: {analysis['interpretation']}")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫
        print("\n" + "=" * 100)
        print("üîÄ –ü–ê–¢–¢–ï–†–ù–´ –û–®–ò–ë–û–ö")
        print("=" * 100)
        
        patterns = interpretation['error_patterns']
        
        if patterns['concentrated_errors']:
            print("\nüìå –ö–û–ù–¶–ï–ù–¢–†–ò–†–û–í–ê–ù–ù–´–ï –û–®–ò–ë–ö–ò (>60% –≤ –æ–¥–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏):")
            for ce in patterns['concentrated_errors']:
                print(f"   {ce['class']} ‚Üí {ce['main_confusion']}: {ce['concentration']:.1f}% –æ—à–∏–±–æ–∫ ({ce['total_errors']:,} –æ—à–∏–±–æ–∫)")
                print(f"     üí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: –û—à–∏–±–∫–∏ —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–¥—É—Ç –≤ –æ–¥–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, —á—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
        
        if patterns['scattered_errors']:
            print("\nüåê –†–ê–ó–ë–†–û–°–ê–ù–ù–´–ï –û–®–ò–ë–ö–ò (<30% –≤ –ª—é–±–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏):")
            for se in patterns['scattered_errors']:
                print(f"   {se['class']}: –æ—à–∏–±–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É {se['error_distribution']} –∫–ª–∞—Å—Å–∞–º–∏ ({se['total_errors']:,} –æ—à–∏–±–æ–∫)")
                print(f"     üí° {se['interpretation']}")
        
        if patterns['bidirectional_confusions']:
            print("\nüîÑ –î–í–£–°–¢–û–†–û–ù–ù–ò–ï –ü–£–¢–ê–ù–ò–¶–´:")
            for bc in patterns['bidirectional_confusions'][:10]:
                print(f"   {bc['class1']} ‚Üî {bc['class2']}:")
                print(f"     {bc['class1']} ‚Üí {bc['class2']}: {bc['count1_to_2']:,} ({bc['pct1_to_2']:.1f}%)")
                print(f"     {bc['class2']} ‚Üí {bc['class1']}: {bc['count2_to_1']:,} ({bc['pct2_to_1']:.1f}%)")
                print(f"     üí° {bc['interpretation']}")
        
        # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤
        print("\n" + "=" * 100)
        print("üìä –°–¢–ê–ë–ò–õ–¨–ù–û–°–¢–¨ –ö–õ–ê–°–°–û–í")
        print("=" * 100)
        print("(–í—ã—Å–æ–∫–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å = –æ—à–∏–±–∫–∏ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É—é—Ç—Å—è –≤ 1-2 –∫–ª–∞—Å—Å–∞—Ö, –ª–µ–≥–∫–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å)")
        print("-" * 100)
        
        for stability in interpretation['class_stability']:
            stability_emoji = 'üü¢' if stability['stability_score'] > 0.7 else 'üü°' if stability['stability_score'] > 0.4 else 'üî¥'
            print(f"{stability_emoji} {stability['class']}: —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å {stability['stability_score']:.3f}")
            print(f"   –û—à–∏–±–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É {stability['unique_error_classes']} –∫–ª–∞—Å—Å–∞–º–∏")
            print(f"   üí° {stability['interpretation']}")
        
        # –ö–ª–∞—Å—Ç–µ—Ä—ã –ø—É—Ç–∞–Ω–∏—Ü
        if interpretation['confusion_clusters']:
            print("\n" + "=" * 100)
            print("üîó –ö–õ–ê–°–¢–ï–†–´ –ü–£–¢–ê–ù–ò–¶ (–∫–ª–∞—Å—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—É—Ç–∞—é—Ç—Å—è –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º)")
            print("=" * 100)
            
            for i, cluster in enumerate(interpretation['confusion_clusters'][:10], 1):
                strength_emoji = {'—Å–∏–ª—å–Ω–∞—è': 'üî¥', '—Å—Ä–µ–¥–Ω—è—è': 'üü†', '—Å–ª–∞–±–∞—è': 'üü°'}
                emoji = strength_emoji.get(cluster['strength'], '‚Ä¢')
                print(f"\n{emoji} –ö–ª–∞—Å—Ç–µ—Ä {i}: {cluster['classes'][0]} ‚Üî {cluster['classes'][1]}")
                print(f"   –°–∏–ª–∞ —Å–≤—è–∑–∏: {cluster['strength']}")
                print(f"   {cluster['classes'][0]} ‚Üí {cluster['classes'][1]}: {cluster['count1_to_2']:,} ({cluster['pct1_to_2']:.1f}%)")
                print(f"   {cluster['classes'][1]} ‚Üí {cluster['classes'][0]}: {cluster['count2_to_1']:,} ({cluster['pct2_to_1']:.1f}%)")
                print(f"   –í—Å–µ–≥–æ –ø—É—Ç–∞–Ω–∏—Ü: {cluster['total_confusions']:,}")
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –æ—à–∏–±–æ–∫
        if 'error_categorization' in interpretation:
            print("\n" + "=" * 100)
            print("üìÇ –ö–ê–¢–ï–ì–û–†–ò–ó–ê–¶–ò–Ø –û–®–ò–ë–û–ö")
            print("=" * 100)
            
            categorization = interpretation['error_categorization']
            
            # –°–≤–æ–¥–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            if categorization.get('category_summary'):
                print("\nüìä –°–í–û–î–ö–ê –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
                print("-" * 100)
                for cat_name, summary in categorization['category_summary'].items():
                    severity_emoji = {
                        '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è': 'üî¥', '–≤—ã—Å–æ–∫–∞—è': 'üü†', 
                        '—Å—Ä–µ–¥–Ω—è—è': 'üü°', '–Ω–∏–∑–∫–∞—è': 'üü¢', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è': 'üîµ'
                    }
                    emoji = severity_emoji.get(summary['severity'], '‚Ä¢')
                    print(f"\n{emoji} {summary['name'].replace('_', ' ').title()}")
                    print(f"   –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: {summary['severity'].upper()}")
                    print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {summary['description']}")
                    print(f"   –ö–ª–∞—Å—Å–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {summary['classes_count']}")
                    if summary['classes']:
                        print(f"   –ö–ª–∞—Å—Å—ã: {', '.join(summary['classes'][:5])}")
                        if len(summary['classes']) > 5:
                            print(f"   ... –∏ –µ—â–µ {len(summary['classes']) - 5} –∫–ª–∞—Å—Å–æ–≤")
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º
            if categorization.get('by_class'):
                print("\nüìã –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ö–õ–ê–°–°–ê–ú:")
                print("-" * 100)
                for cls, categories in categorization['by_class'].items():
                    if categories:
                        print(f"\n  {cls}:")
                        for cat_name in categories:
                            cat = self.category_manager.get_category(cat_name) if self.category_manager else None
                            if cat:
                                print(f"    ‚Ä¢ {cat_name.replace('_', ' ').title()} ({cat.severity.value})")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if interpretation['recommendations']:
            print("\n" + "=" * 100)
            print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ")
            print("=" * 100)
            
            for i, rec in enumerate(interpretation['recommendations'], 1):
                priority_emoji = {'–∫—Ä–∏—Ç–∏—á–Ω—ã–π': 'üî¥', '–≤—ã—Å–æ–∫–∏–π': 'üü†', '—Å—Ä–µ–¥–Ω–∏–π': 'üü°', '–Ω–∏–∑–∫–∏–π': 'üü¢'}
                print(f"\n{i}. {priority_emoji.get(rec['priority'], '‚Ä¢')} [{rec['priority'].upper()}] {rec['type'].replace('_', ' ').title()}")
                print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {rec['description']}")
                print(f"   –î–µ–π—Å—Ç–≤–∏–µ: {rec['action']}")
        
        print("\n" + "=" * 100)
    
    def save_report(self, output_file='CONFUSION_MATRIX_REPORT.md', include_visualizations=True):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª
        
        Args:
            output_file: –ø—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
            include_visualizations: —Å–æ–∑–¥–∞–≤–∞—Ç—å –ª–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –¥–æ–±–∞–≤–ª—è—Ç—å —Å—Å—ã–ª–∫–∏ –≤ –æ—Ç—á–µ—Ç
        """
        if self.confusion_matrix is None:
            print("‚ùå –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞")
            return
        
        metrics = self.calculate_metrics_from_matrix()
        normalized_matrix = self.get_normalized_matrix()
        mistakes = self.find_common_mistakes(top_n=15)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        vis_paths = None
        if include_visualizations:
            try:
                vis_paths = self.create_all_visualizations(show=False)
            except Exception as e:
                print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
                vis_paths = None
        
        report_lines = [
            "# üìä –û–¢–ß–ï–¢ –ü–û CONFUSION MATRIX",
            "",
            f"**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "---",
            ""
        ]
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        if vis_paths:
            report_lines.extend([
                "## üé® –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏",
                "",
                "–ì—Ä–∞—Ñ–∏–∫–∏ –∏ –¥–∏–∞–≥—Ä–∞–º–º—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ `confusion_matrix_figures/`:",
                "",
                f"- **Confusion Matrix (–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)**: `{os.path.basename(vis_paths['confusion_matrix_absolute'])}`",
                f"- **Confusion Matrix (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è)**: `{os.path.basename(vis_paths['confusion_matrix_normalized'])}`",
                f"- **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫**: `{os.path.basename(vis_paths['metrics_comparison'])}`",
                f"- **–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫**: `{os.path.basename(vis_paths['error_analysis'])}`",
                "",
                "---",
                ""
            ])
        
        report_lines.extend([
            "## üìä Confusion Matrix (–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)",
            "",
            "**–°—Ç—Ä–æ–∫–∏ = –ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã | –ö–æ–ª–æ–Ω–∫–∏ = –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã**",
            "",
            "| True / Pred | " + " | ".join(self.classes) + " |",
            "|" + "|".join(["---"] * (len(self.classes) + 1)) + "|"
        ])
        
        # –ê–±—Å–æ–ª—é—Ç–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
        for true_cls in self.classes:
            row_values = [str(int(self.confusion_matrix.loc[true_cls, pred_cls])) for pred_cls in self.classes]
            report_lines.append(f"| **{true_cls}** | " + " | ".join(row_values) + " |")
        
        report_lines.extend([
            "",
            "## üìä –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è Confusion Matrix (%)",
            "",
            "**–ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (—Å—Ç—Ä–æ–∫–∏ = 100%)**",
            "",
            "| True / Pred | " + " | ".join(self.classes) + " |",
            "|" + "|".join(["---"] * (len(self.classes) + 1)) + "|"
        ])
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
        for true_cls in self.classes:
            row_values = [f"{normalized_matrix.loc[true_cls, pred_cls]:.1f}%" for pred_cls in self.classes]
            report_lines.append(f"| **{true_cls}** | " + " | ".join(row_values) + " |")
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        report_lines.extend([
            "",
            "## üìã –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º",
            "",
            "| –ö–ª–∞—Å—Å | Precision | Recall | F1-Score | Support |",
            "|-------|-----------|--------|----------|---------|"
        ])
        
        for cls in self.classes:
            m = metrics[cls]
            report_lines.append(
                f"| {cls} | {m['precision']:.4f} | {m['recall']:.4f} | "
                f"{m['f1']:.4f} | {m['support']} |"
            )
        
        # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        macro_precision = np.mean([m['precision'] for m in metrics.values()])
        macro_recall = np.mean([m['recall'] for m in metrics.values()])
        macro_f1 = np.mean([m['f1'] for m in metrics.values()])
        
        total_tp = sum(m['tp'] for m in metrics.values())
        total_fp = sum(m['fp'] for m in metrics.values())
        total_fn = sum(m['fn'] for m in metrics.values())
        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0
        
        total_samples = self.confusion_matrix.values.sum()
        correct = sum(self.confusion_matrix.loc[cls, cls] for cls in self.classes)
        accuracy = (correct / total_samples * 100) if total_samples > 0 else 0
        
        report_lines.extend([
            "",
            "## üìä –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏",
            "",
            "| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ |",
            "|---------|----------|",
            f"| **Macro Precision** | {macro_precision:.4f} |",
            f"| **Macro Recall** | {macro_recall:.4f} |",
            f"| **Macro F1** | {macro_f1:.4f} |",
            f"| **Micro Precision** | {micro_precision:.4f} |",
            f"| **Micro Recall** | {micro_recall:.4f} |",
            f"| **Micro F1** | {micro_f1:.4f} |",
            f"| **–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy)** | {accuracy:.2f}% ({correct:,}/{total_samples:,}) |",
            "",
            "## ‚ö†Ô∏è –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏",
            "",
            "| –ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | % –æ—Ç –∫–ª–∞—Å—Å–∞ |",
            "|----------------|---------------------|------------|-------------|"
        ])
        
        for true_cls, pred_cls, count in mistakes:
            total_true = self.confusion_matrix.loc[true_cls, :].sum()
            percentage = (count / total_true * 100) if total_true > 0 else 0
            report_lines.append(f"| {true_cls} | {pred_cls} | {count} | {percentage:.2f}% |")
        
        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –æ—à–∏–±–æ–∫
        interpretation = self.interpret_errors()
        
        report_lines.extend([
            "",
            "## üîç –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –û–®–ò–ë–û–ö",
            ""
        ])
        
        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        if interpretation['problematic_classes']:
            report_lines.extend([
                "### ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–ª–∞—Å—Å—ã (–±–æ–ª–µ–µ 50% –æ—à–∏–±–æ–∫)",
                ""
            ])
            for pc in interpretation['problematic_classes']:
                report_lines.append(f"#### üìå {pc['class']}")
                report_lines.append(f"- **–û—à–∏–±–æ–∫**: {pc['error_rate']}% ({pc['total'] - pc['correct']:,} –∏–∑ {pc['total']:,})")
                report_lines.append(f"- **–ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö**: {pc['correct']:,} ({100 - pc['error_rate']:.1f}%)")
                if pc['main_confusions']:
                    report_lines.append("- **–û—Å–Ω–æ–≤–Ω—ã–µ –ø—É—Ç–∞–Ω–∏—Ü—ã**:")
                    for conf in pc['main_confusions']:
                        report_lines.append(f"  - `{conf['confused_with']}`: {conf['count']:,} ({conf['percentage']}%)")
                report_lines.append("")
        
        # –ö–ª–∞—Å—Å—ã —Å –Ω–∏–∑–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é
        if interpretation['low_performance_classes']:
            report_lines.extend([
                "### üìâ –ö–ª–∞—Å—Å—ã —Å –Ω–∏–∑–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é",
                "",
                "| –ö–ª–∞—Å—Å | Precision | Recall | F1-Score | –ü—Ä–æ–±–ª–µ–º—ã |",
                "|-------|-----------|--------|----------|---------|"
            ])
            for lpc in interpretation['low_performance_classes']:
                issues_str = "; ".join(lpc['issues'])
                report_lines.append(
                    f"| {lpc['class']} | {lpc['precision']:.2%} | {lpc['recall']:.2%} | "
                    f"{lpc['f1']:.2%} | {issues_str} |"
                )
            report_lines.append("")
        
        # –°–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏
        if interpretation['symmetric_errors']:
            report_lines.extend([
                "### üîÑ –°–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ (–∫–ª–∞—Å—Å—ã –ø—É—Ç–∞—é—Ç—Å—è –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º)",
                "",
                "**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**: –≠—Ç–∏ –∫–ª–∞—Å—Å—ã –∏–º–µ—é—Ç –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∏",
                "",
                "| –ö–ª–∞—Å—Å 1 | –ö–ª–∞—Å—Å 2 | 1‚Üí2 –æ—à–∏–±–æ–∫ | 2‚Üí1 –æ—à–∏–±–æ–∫ | –í—Å–µ–≥–æ |",
                "|---------|---------|------------|------------|-------|"
            ])
            for se in interpretation['symmetric_errors']:
                report_lines.append(
                    f"| {se['class1']} | {se['class2']} | {se['count1_to_2']:,} | "
                    f"{se['count2_to_1']:,} | {se['total_mistakes']:,} |"
                )
            report_lines.append("")
        
        # –î–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ –æ—à–∏–±–∫–∏
        if interpretation['dominant_confusions']:
            report_lines.extend([
                "### üìä –î–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ –æ—à–∏–±–∫–∏ (–±–æ–ª–µ–µ 20% –æ—Ç –∫–ª–∞—Å—Å–∞)",
                "",
                "| –ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | % –æ—Ç –∫–ª–∞—Å—Å–∞ | –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å |",
                "|----------------|---------------------|------------|-------------|--------------|"
            ])
            for dc in sorted(interpretation['dominant_confusions'], key=lambda x: x['percentage'], reverse=True):
                report_lines.append(
                    f"| {dc['true_class']} | {dc['predicted_class']} | {dc['count']:,} | "
                    f"{dc['percentage']}% | {dc['severity']} |"
                )
            report_lines.append("")
        
        # –ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –æ—à–∏–±–æ–∫
        report_lines.extend([
            "",
            "## üîç –ü–û–õ–ù–ê–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –û–®–ò–ë–û–ö",
            ""
        ])
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫
        stats = interpretation['error_statistics']
        report_lines.extend([
            "### üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫",
            "",
            f"- **–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤**: {stats['total_samples']:,}",
            f"- **–ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π**: {stats['total_correct']:,} ({100 - stats['overall_error_rate']:.2f}%)",
            f"- **–û—à–∏–±–æ–∫**: {stats['total_errors']:,} ({stats['overall_error_rate']:.2f}%)",
            f"- **–°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º**: {stats['average_error_rate_per_class']:.2f}% (œÉ={stats['error_rate_std']:.2f}%)",
            ""
        ])
        
        if stats['top_error_contributors']:
            report_lines.extend([
                "#### üî¥ –¢–æ–ø-5 –∫–ª–∞—Å—Å–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –≤–∫–ª–∞–¥–æ–º –≤ –æ—à–∏–±–∫–∏:",
                "",
                "| –ö–ª–∞—Å—Å | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ | % –æ—Ç –≤—Å–µ—Ö –æ—à–∏–±–æ–∫ |",
                "|-------|-------------------|------------------|"
            ])
            for contrib in stats['top_error_contributors']:
                report_lines.append(f"| {contrib['class']} | {contrib['errors']:,} | {contrib['contribution_pct']:.1f}% |")
            report_lines.append("")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Å–∞–º
        detailed = interpretation['detailed_analysis']
        report_lines.extend([
            "### üìã –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É",
            ""
        ])
        
        for cls in sorted(detailed.keys(), key=lambda x: detailed[x]['severity'] == '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è', reverse=True):
            analysis = detailed[cls]
            severity_emoji = {'–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è': 'üî¥', '–≤—ã—Å–æ–∫–∞—è': 'üü†', '—Å—Ä–µ–¥–Ω—è—è': 'üü°', '–Ω–∏–∑–∫–∞—è': 'üü¢'}
            emoji = severity_emoji.get(analysis['severity'], '‚Ä¢')
            
            report_lines.extend([
                f"#### {emoji} {cls}",
                "",
                f"- **–°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º**: {analysis['severity'].upper()}",
                f"- **–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤**: {analysis['total_samples']:,}",
                f"- **–ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö**: {analysis['correct_predictions']:,} ({100 - analysis['error_rate']:.1f}%)",
                f"- **–û—à–∏–±–æ–∫**: {analysis['error_count']:,} ({analysis['error_rate']:.1f}%)",
                f"- **Precision**: {analysis['metrics']['precision']:.3f} | **Recall**: {analysis['metrics']['recall']:.3f} | **F1**: {analysis['metrics']['f1']:.3f}",
                ""
            ])
            
            if analysis['issues']:
                report_lines.append(f"- **–ü—Ä–æ–±–ª–µ–º—ã**: {', '.join(analysis['issues'])}")
                report_lines.append("")
            
            if analysis['most_confused_with']:
                report_lines.append("- **–û—Å–Ω–æ–≤–Ω—ã–µ –ø—É—Ç–∞–Ω–∏—Ü—ã**:")
                for conf in analysis['most_confused_with'][:3]:
                    report_lines.append(f"  - `{conf['class']}`: {conf['count']:,} ({conf['percentage']:.1f}%)")
                report_lines.append("")
            
            report_lines.extend([
                f"**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**: {analysis['interpretation']}",
                ""
            ])
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫
        patterns = interpretation['error_patterns']
        report_lines.extend([
            "### üîÄ –ü–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫",
            ""
        ])
        
        if patterns['concentrated_errors']:
            report_lines.extend([
                "#### üìå –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ (>60% –≤ –æ–¥–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏)",
                "",
                "| –ö–ª–∞—Å—Å | –û—Å–Ω–æ–≤–Ω–∞—è –ø—É—Ç–∞–Ω–∏—Ü–∞ | –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è | –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫ |",
                "|-------|-------------------|--------------|--------------|"
            ])
            for ce in patterns['concentrated_errors']:
                report_lines.append(
                    f"| {ce['class']} | {ce['main_confusion']} | {ce['concentration']:.1f}% | {ce['total_errors']:,} |"
                )
            report_lines.append("")
            report_lines.append("*–û—à–∏–±–∫–∏ —ç—Ç–∏—Ö –∫–ª–∞—Å—Å–æ–≤ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–¥—É—Ç –≤ –æ–¥–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, —á—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ*")
            report_lines.append("")
        
        if patterns['scattered_errors']:
            report_lines.extend([
                "#### üåê –†–∞–∑–±—Ä–æ—Å–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ (<30% –≤ –ª—é–±–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏)",
                "",
                "| –ö–ª–∞—Å—Å | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏ | –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫ |",
                "|-------|-------------------------------|--------------|"
            ])
            for se in patterns['scattered_errors']:
                report_lines.append(
                    f"| {se['class']} | {se['error_distribution']} | {se['total_errors']:,} |"
                )
            report_lines.append("")
            report_lines.append("*–û—à–∏–±–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É –º–Ω–æ–≥–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –æ–±—â—É—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏*")
            report_lines.append("")
        
        if patterns['bidirectional_confusions']:
            report_lines.extend([
                "#### üîÑ –î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ –ø—É—Ç–∞–Ω–∏—Ü—ã",
                "",
                "| –ö–ª–∞—Å—Å 1 | –ö–ª–∞—Å—Å 2 | 1‚Üí2 –æ—à–∏–±–æ–∫ | 2‚Üí1 –æ—à–∏–±–æ–∫ | –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è |",
                "|---------|---------|------------|------------|---------------|"
            ])
            for bc in patterns['bidirectional_confusions'][:10]:
                report_lines.append(
                    f"| {bc['class1']} | {bc['class2']} | {bc['count1_to_2']:,} ({bc['pct1_to_2']:.1f}%) | "
                    f"{bc['count2_to_1']:,} ({bc['pct2_to_1']:.1f}%) | {bc['interpretation']} |"
                )
            report_lines.append("")
        
        # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤
        report_lines.extend([
            "### üìä –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤",
            "",
            "*(–í—ã—Å–æ–∫–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å = –æ—à–∏–±–∫–∏ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É—é—Ç—Å—è –≤ 1-2 –∫–ª–∞—Å—Å–∞—Ö, –ª–µ–≥–∫–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å)*",
            "",
            "| –ö–ª–∞—Å—Å | –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å | –ö–ª–∞—Å—Å–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏ | –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è |",
            "|-------|--------------|-------------------|---------------|"
        ])
        
        for stability in interpretation['class_stability']:
            report_lines.append(
                f"| {stability['class']} | {stability['stability_score']:.3f} | {stability['unique_error_classes']} | {stability['interpretation']} |"
            )
        report_lines.append("")
        
        # –ö–ª–∞—Å—Ç–µ—Ä—ã –ø—É—Ç–∞–Ω–∏—Ü
        if interpretation['confusion_clusters']:
            report_lines.extend([
                "### üîó –ö–ª–∞—Å—Ç–µ—Ä—ã –ø—É—Ç–∞–Ω–∏—Ü",
                "",
                "*(–ö–ª–∞—Å—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—É—Ç–∞—é—Ç—Å—è –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º)*",
                "",
                "| –ö–ª–∞—Å—Å 1 | –ö–ª–∞—Å—Å 2 | 1‚Üí2 | 2‚Üí1 | –í—Å–µ–≥–æ | –°–∏–ª–∞ —Å–≤—è–∑–∏ |",
                "|---------|---------|-----|-----|-------|------------|"
            ])
            for cluster in interpretation['confusion_clusters'][:15]:
                report_lines.append(
                    f"| {cluster['classes'][0]} | {cluster['classes'][1]} | "
                    f"{cluster['count1_to_2']:,} ({cluster['pct1_to_2']:.1f}%) | "
                    f"{cluster['count2_to_1']:,} ({cluster['pct2_to_1']:.1f}%) | "
                    f"{cluster['total_confusions']:,} | {cluster['strength']} |"
                )
            report_lines.append("")
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –æ—à–∏–±–æ–∫
        if 'error_categorization' in interpretation:
            categorization = interpretation['error_categorization']
            
            report_lines.extend([
                "",
                "## üìÇ –ö–ê–¢–ï–ì–û–†–ò–ó–ê–¶–ò–Ø –û–®–ò–ë–û–ö",
                ""
            ])
            
            # –°–≤–æ–¥–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            if categorization.get('category_summary'):
                report_lines.extend([
                    "### üìä –°–≤–æ–¥–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º",
                    "",
                    "| –ö–∞—Ç–µ–≥–æ—Ä–∏—è | –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å | –ö–ª–∞—Å—Å–æ–≤ | –û–ø–∏—Å–∞–Ω–∏–µ |",
                    "|-----------|-------------|---------|----------|"
                ])
                
                for cat_name, summary in categorization['category_summary'].items():
                    classes_str = ', '.join(summary['classes'][:3])
                    if len(summary['classes']) > 3:
                        classes_str += f" (+{len(summary['classes']) - 3})"
                    report_lines.append(
                        f"| {summary['name'].replace('_', ' ').title()} | {summary['severity']} | "
                        f"{summary['classes_count']} | {summary['description']} |"
                    )
                report_lines.append("")
                
                # –î–µ—Ç–∞–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
                for cat_name, summary in categorization['category_summary'].items():
                    if summary['classes_count'] > 0:
                        report_lines.extend([
                            f"#### {summary['name'].replace('_', ' ').title()}",
                            "",
                            f"**–°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å**: {summary['severity']}",
                            f"**–ö–ª–∞—Å—Å–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏**: {summary['classes_count']}",
                            f"**–ö–ª–∞—Å—Å—ã**: {', '.join(summary['classes'])}",
                            "",
                            "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏**:",
                            ""
                        ])
                        for rec in summary['recommendations']:
                            report_lines.append(f"- {rec}")
            report_lines.append("")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if interpretation['recommendations']:
            report_lines.extend([
                "### üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é",
                ""
            ])
            for i, rec in enumerate(interpretation['recommendations'], 1):
                report_lines.extend([
                    f"#### {i}. [{rec['priority'].upper()}] {rec['type'].replace('_', ' ').title()}",
                    "",
                    f"**–û–ø–∏—Å–∞–Ω–∏–µ**: {rec['description']}",
                    "",
                    f"**–î–µ–π—Å—Ç–≤–∏–µ**: {rec['action']}",
                    ""
                ])
        
        report_lines.extend([
            "",
            f"**–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤**: {total_samples:,}",
            f"**–í—Å–µ–≥–æ –∫–ª–∞—Å—Å–æ–≤**: {len(self.classes)}",
            ""
        ])
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è—Ö –≤ –∫–æ–Ω–µ—Ü
        if vis_paths:
            report_lines.extend([
                "",
                "---",
                "",
                "## üìÅ –§–∞–π–ª—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π",
                "",
                "–í—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –≤—ã—Å–æ–∫–æ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ (300 DPI) –∏ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –æ—Ç—á–µ—Ç–∞–º–∏."
        ])
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –æ—à–∏–±–æ–∫ –µ—Å–ª–∏ –µ—Å—Ç—å
        if self.category_manager:
            categories_file = output_file.replace('.md', '_categories.json')
            self.category_manager.save_categories(categories_file)
            print(f"‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {categories_file}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON
        json_data = {
            'confusion_matrix': self.confusion_matrix.to_dict(),
            'normalized_matrix': normalized_matrix.to_dict(),
            'metrics': metrics,
            'common_mistakes': [{'true_class': t, 'pred_class': p, 'count': c} for t, p, c in mistakes],
            'interpretation': interpretation,
            'overall_metrics': {
                'macro_precision': float(macro_precision),
                'macro_recall': float(macro_recall),
                'macro_f1': float(macro_f1),
                'micro_precision': float(micro_precision),
                'micro_recall': float(micro_recall),
                'micro_f1': float(micro_f1),
                'accuracy': float(accuracy),
                'total_samples': int(total_samples),
                'correct_predictions': int(correct)
            }
        }
        
        json_file = output_file.replace('.md', '.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ JSON –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {json_file}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ CSV –º–∞—Ç—Ä–∏—Ü—ã
        csv_file = output_file.replace('.md', '.csv')
        self.confusion_matrix.to_csv(csv_file)
        print(f"‚úÖ CSV –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {csv_file}")
    
    def save_html_report(self, output_file='CONFUSION_MATRIX_REPORT.html', include_visualizations=True):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ HTML-–æ—Ç—á–µ—Ç–∞ —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π –æ—à–∏–±–æ–∫ –¥–ª—è –≤–µ–±-–ø—Ä–æ—Å–º–æ—Ç—Ä–∞
        
        Args:
            output_file: –ø—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É HTML —Ñ–∞–π–ª—É
            include_visualizations: —Å–æ–∑–¥–∞–≤–∞—Ç—å –ª–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –¥–æ–±–∞–≤–ª—è—Ç—å —Å—Å—ã–ª–∫–∏
        """
        if self.confusion_matrix is None:
            print("‚ùå –ú–∞—Ç—Ä–∏—Ü–∞ –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞")
            return
        
        metrics = self.calculate_metrics_from_matrix()
        normalized_matrix = self.get_normalized_matrix()
        mistakes = self.find_common_mistakes(top_n=20)
        interpretation = self.interpret_errors()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        vis_paths = None
        if include_visualizations:
            try:
                vis_paths = self.create_all_visualizations(show=False)
            except Exception as e:
                print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
                vis_paths = None
        
        # –†–∞—Å—á–µ—Ç –æ–±—â–∏—Ö –º–µ—Ç—Ä–∏–∫
        macro_precision = np.mean([m['precision'] for m in metrics.values()])
        macro_recall = np.mean([m['recall'] for m in metrics.values()])
        macro_f1 = np.mean([m['f1'] for m in metrics.values()])
        
        total_tp = sum(m['tp'] for m in metrics.values())
        total_fp = sum(m['fp'] for m in metrics.values())
        total_fn = sum(m['fn'] for m in metrics.values())
        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0
        
        total_samples = self.confusion_matrix.values.sum()
        correct = sum(self.confusion_matrix.loc[cls, cls] for cls in self.classes)
        accuracy = (correct / total_samples * 100) if total_samples > 0 else 0
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML
        html_content = self._generate_html_content(
            metrics, normalized_matrix, mistakes, interpretation,
            macro_precision, macro_recall, macro_f1,
            micro_precision, micro_recall, micro_f1,
            accuracy, total_samples, correct, vis_paths
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"‚úÖ HTML –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
        print(f"üìÇ –û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª –≤ –±—Ä–∞—É–∑–µ—Ä–µ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –æ—à–∏–±–æ–∫")
        
        return output_file
    
    def _generate_html_content(self, metrics, normalized_matrix, mistakes, interpretation,
                               macro_precision, macro_recall, macro_f1,
                               micro_precision, micro_recall, micro_f1,
                               accuracy, total_samples, correct, vis_paths):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        # CSS —Å—Ç–∏–ª–∏
        css_styles = """
        <style>
            * { margin: 0; padding: 0; box-sizing: border-box; }
            body {
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                line-height: 1.6;
                color: #333;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                padding: 20px;
            }
            .container {
                max-width: 1400px;
                margin: 0 auto;
                background: white;
                border-radius: 15px;
                box-shadow: 0 10px 40px rgba(0,0,0,0.2);
                padding: 40px;
            }
            h1 {
                color: #667eea;
                text-align: center;
                margin-bottom: 10px;
                font-size: 2.5em;
            }
            .subtitle {
                text-align: center;
                color: #666;
                margin-bottom: 30px;
                font-size: 1.1em;
            }
            h2 {
                color: #764ba2;
                margin-top: 40px;
                margin-bottom: 20px;
                padding-bottom: 10px;
                border-bottom: 3px solid #667eea;
                font-size: 1.8em;
            }
            h3 {
                color: #555;
                margin-top: 30px;
                margin-bottom: 15px;
                font-size: 1.4em;
            }
            h4 {
                color: #666;
                margin-top: 20px;
                margin-bottom: 10px;
                font-size: 1.2em;
            }
            .stats-grid {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
                gap: 20px;
                margin: 30px 0;
            }
            .stat-card {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 25px;
                border-radius: 10px;
                text-align: center;
                box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            }
            .stat-card h3 {
                color: white;
                margin: 0 0 10px 0;
                font-size: 1.1em;
            }
            .stat-value {
                font-size: 2.5em;
                font-weight: bold;
                margin: 10px 0;
            }
            table {
                width: 100%;
                border-collapse: collapse;
                margin: 20px 0;
                background: white;
                box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            }
            th {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 15px;
                text-align: left;
                font-weight: 600;
            }
            td {
                padding: 12px 15px;
                border-bottom: 1px solid #eee;
            }
            tr:hover {
                background: #f8f9fa;
            }
            .severity-critical { background-color: #ffebee; border-left: 4px solid #f44336; }
            .severity-high { background-color: #fff3e0; border-left: 4px solid #ff9800; }
            .severity-medium { background-color: #fff9c4; border-left: 4px solid #ffc107; }
            .severity-low { background-color: #e8f5e9; border-left: 4px solid #4caf50; }
            .badge {
                display: inline-block;
                padding: 5px 12px;
                border-radius: 20px;
                font-size: 0.85em;
                font-weight: 600;
                margin: 2px;
            }
            .badge-critical { background: #f44336; color: white; }
            .badge-high { background: #ff9800; color: white; }
            .badge-medium { background: #ffc107; color: white; }
            .badge-low { background: #4caf50; color: white; }
            .interpretation-box {
                background: #f8f9fa;
                padding: 20px;
                border-radius: 8px;
                margin: 15px 0;
                border-left: 4px solid #667eea;
            }
            .pattern-card {
                background: white;
                padding: 20px;
                margin: 15px 0;
                border-radius: 8px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.1);
                border-left: 4px solid #667eea;
            }
            .cluster-item {
                background: #f8f9fa;
                padding: 15px;
                margin: 10px 0;
                border-radius: 8px;
                border-left: 4px solid #764ba2;
            }
            .recommendation {
                background: #e3f2fd;
                padding: 20px;
                margin: 15px 0;
                border-radius: 8px;
                border-left: 4px solid #2196f3;
            }
            .metric-bar {
                height: 25px;
                background: #e0e0e0;
                border-radius: 12px;
                margin: 5px 0;
                overflow: hidden;
            }
            .metric-fill {
                height: 100%;
                background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
                display: flex;
                align-items: center;
                justify-content: center;
                color: white;
                font-weight: 600;
                font-size: 0.9em;
            }
            .nav-menu {
                position: sticky;
                top: 20px;
                background: white;
                padding: 15px;
                border-radius: 10px;
                box-shadow: 0 2px 10px rgba(0,0,0,0.1);
                margin-bottom: 30px;
            }
            .nav-menu a {
                display: inline-block;
                padding: 8px 15px;
                margin: 5px;
                background: #667eea;
                color: white;
                text-decoration: none;
                border-radius: 5px;
                transition: background 0.3s;
            }
            .nav-menu a:hover {
                background: #764ba2;
            }
            .section {
                scroll-margin-top: 100px;
            }
            @media print {
                body { background: white; }
                .container { box-shadow: none; }
                .nav-menu { display: none; }
            }
        </style>
        """
        
        # JavaScript –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        js_script = """
        <script>
            function scrollToSection(id) {
                document.getElementById(id).scrollIntoView({ behavior: 'smooth' });
            }
            function toggleSection(id) {
                const element = document.getElementById(id);
                if (element.style.display === 'none') {
                    element.style.display = 'block';
                } else {
                    element.style.display = 'none';
                }
            }
        </script>
        """
        
        # –ù–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω–æ–µ –º–µ–Ω—é
        nav_menu = """
        <div class="nav-menu">
            <a href="#overview">–û–±–∑–æ—Ä</a>
            <a href="#matrix">–ú–∞—Ç—Ä–∏—Ü–∞</a>
            <a href="#metrics">–ú–µ—Ç—Ä–∏–∫–∏</a>
            <a href="#detailed">–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑</a>
            <a href="#patterns">–ü–∞—Ç—Ç–µ—Ä–Ω—ã</a>
            <a href="#stability">–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</a>
            <a href="#clusters">–ö–ª–∞—Å—Ç–µ—Ä—ã</a>
            <a href="#categorization">–ö–∞—Ç–µ–≥–æ—Ä–∏–∏</a>
            <a href="#recommendations">–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</a>
        </div>
        """
        
        # –ù–∞—á–∞–ª–æ HTML
        html = f"""<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>–ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –æ—à–∏–±–æ–∫ - Confusion Matrix</title>
    {css_styles}
</head>
<body>
    <div class="container">
        <h1>üìä –ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏</h1>
        <div class="subtitle">–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>
        
        {nav_menu}
        
        <!-- –û–ë–ó–û–† -->
        <section id="overview" class="section">
            <h2>üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞</h2>
            <div class="stats-grid">
                <div class="stat-card">
                    <h3>–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤</h3>
                    <div class="stat-value">{total_samples:,}</div>
                </div>
                <div class="stat-card">
                    <h3>–ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö</h3>
                    <div class="stat-value">{correct:,}</div>
                    <div>({100-accuracy:.2f}%)</div>
                </div>
                <div class="stat-card">
                    <h3>–û—à–∏–±–æ–∫</h3>
                    <div class="stat-value">{total_samples-correct:,}</div>
                    <div>({(total_samples-correct)/total_samples*100:.2f}%)</div>
                </div>
                <div class="stat-card">
                    <h3>–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å</h3>
                    <div class="stat-value">{accuracy:.2f}%</div>
                </div>
            </div>
        </section>
        
        <!-- CONFUSION MATRIX -->
        <section id="matrix" class="section">
            <h2>üìä Confusion Matrix</h2>
            {self._generate_matrix_html_table()}
            <h3>–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (%)</h3>
            {self._generate_normalized_matrix_html_table(normalized_matrix)}
        </section>
        
        <!-- –ú–ï–¢–†–ò–ö–ò -->
        <section id="metrics" class="section">
            <h2>üìã –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º</h2>
            {self._generate_metrics_html_table(metrics)}
            
            <h3>–û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏</h3>
            <div class="stats-grid">
                <div class="stat-card">
                    <h3>Macro Precision</h3>
                    <div class="stat-value">{macro_precision:.4f}</div>
                </div>
                <div class="stat-card">
                    <h3>Macro Recall</h3>
                    <div class="stat-value">{macro_recall:.4f}</div>
                </div>
                <div class="stat-card">
                    <h3>Macro F1</h3>
                    <div class="stat-value">{macro_f1:.4f}</div>
                </div>
                <div class="stat-card">
                    <h3>Micro F1</h3>
                    <div class="stat-value">{micro_f1:.4f}</div>
                </div>
            </div>
        </section>
        
        <!-- –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó -->
        <section id="detailed" class="section">
            <h2>üîç –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É</h2>
            {self._generate_detailed_analysis_html(interpretation['detailed_analysis'])}
        </section>
        
        <!-- –ü–ê–¢–¢–ï–†–ù–´ –û–®–ò–ë–û–ö -->
        <section id="patterns" class="section">
            <h2>üîÄ –ü–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫</h2>
            {self._generate_patterns_html(interpretation['error_patterns'])}
        </section>
        
        <!-- –°–¢–ê–ë–ò–õ–¨–ù–û–°–¢–¨ -->
        <section id="stability" class="section">
            <h2>üìä –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤</h2>
            {self._generate_stability_html(interpretation['class_stability'])}
        </section>
        
        <!-- –ö–õ–ê–°–¢–ï–†–´ -->
        <section id="clusters" class="section">
            <h2>üîó –ö–ª–∞—Å—Ç–µ—Ä—ã –ø—É—Ç–∞–Ω–∏—Ü</h2>
            {self._generate_clusters_html(interpretation['confusion_clusters'])}
        </section>
        
        <!-- –ö–ê–¢–ï–ì–û–†–ò–ó–ê–¶–ò–Ø –û–®–ò–ë–û–ö -->
        {self._generate_categorization_html(interpretation.get('error_categorization', {})) if 'error_categorization' in interpretation else ''}
        
        <!-- –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò -->
        <section id="recommendations" class="section">
            <h2>üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é</h2>
            {self._generate_recommendations_html(interpretation['recommendations'])}
        </section>
        
        <!-- –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò -->
        {self._generate_visualizations_section(vis_paths) if vis_paths else ''}
        
    </div>
    {js_script}
</body>
</html>"""
        
        return html
    
    def _generate_matrix_html_table(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è confusion matrix"""
        html = '<table><thead><tr><th>True / Pred</th>'
        for cls in self.classes:
            html += f'<th>{cls}</th>'
        html += '</tr></thead><tbody>'
        
        for true_cls in self.classes:
            html += f'<tr><th>{true_cls}</th>'
            for pred_cls in self.classes:
                value = int(self.confusion_matrix.loc[true_cls, pred_cls])
                # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–∏
                if true_cls == pred_cls:
                    html += f'<td style="background-color: #c8e6c9; font-weight: bold;">{value:,}</td>'
                else:
                    html += f'<td>{value:,}</td>'
            html += '</tr>'
        
        html += '</tbody></table>'
        return html
    
    def _generate_normalized_matrix_html_table(self, normalized_matrix):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã"""
        html = '<table><thead><tr><th>True / Pred</th>'
        for cls in self.classes:
            html += f'<th>{cls}</th>'
        html += '</tr></thead><tbody>'
        
        for true_cls in self.classes:
            html += f'<tr><th>{true_cls}</th>'
            for pred_cls in self.classes:
                value = normalized_matrix.loc[true_cls, pred_cls]
                # –¶–≤–µ—Ç–æ–≤–∞—è –∏–Ω–¥–∏–∫–∞—Ü–∏—è
                if true_cls == pred_cls:
                    color = '#c8e6c9' if value > 50 else '#fff9c4' if value > 30 else '#ffccbc'
                    html += f'<td style="background-color: {color}; font-weight: bold;">{value:.1f}%</td>'
                else:
                    color = '#ffccbc' if value > 20 else '#fff9c4' if value > 10 else '#f5f5f5'
                    html += f'<td style="background-color: {color};">{value:.1f}%</td>'
            html += '</tr>'
        
        html += '</tbody></table>'
        return html
    
    def _generate_metrics_html_table(self, metrics):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML —Ç–∞–±–ª–∏—Ü—ã –º–µ—Ç—Ä–∏–∫"""
        html = '<table><thead><tr><th>–ö–ª–∞—Å—Å</th><th>Precision</th><th>Recall</th><th>F1-Score</th><th>Support</th></tr></thead><tbody>'
        
        for cls in self.classes:
            m = metrics[cls]
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
            if m['f1'] < 0.5:
                severity_class = 'severity-critical'
            elif m['f1'] < 0.7:
                severity_class = 'severity-high'
            elif m['f1'] < 0.85:
                severity_class = 'severity-medium'
            else:
                severity_class = 'severity-low'
            
            html += f'<tr class="{severity_class}">'
            html += f'<td><strong>{cls}</strong></td>'
            html += f'<td>{m["precision"]:.4f}</td>'
            html += f'<td>{m["recall"]:.4f}</td>'
            html += f'<td><strong>{m["f1"]:.4f}</strong></td>'
            html += f'<td>{m["support"]:,}</td>'
            html += '</tr>'
        
        html += '</tbody></table>'
        return html
    
    def _generate_detailed_analysis_html(self, detailed_analysis):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        html = ''
        
        for cls in sorted(detailed_analysis.keys(), 
                         key=lambda x: detailed_analysis[x]['severity'] == '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è', 
                         reverse=True):
            analysis = detailed_analysis[cls]
            severity_class = {
                '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è': 'severity-critical',
                '–≤—ã—Å–æ–∫–∞—è': 'severity-high',
                '—Å—Ä–µ–¥–Ω—è—è': 'severity-medium',
                '–Ω–∏–∑–∫–∞—è': 'severity-low'
            }.get(analysis['severity'], '')
            
            html += f'<div class="pattern-card {severity_class}">'
            html += f'<h4>{cls}</h4>'
            html += f'<p><strong>–°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å:</strong> <span class="badge badge-{analysis["severity"]}">{analysis["severity"].upper()}</span></p>'
            html += f'<p><strong>–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤:</strong> {analysis["total_samples"]:,} | '
            html += f'<strong>–ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö:</strong> {analysis["correct_predictions"]:,} ({100-analysis["error_rate"]:.1f}%) | '
            html += f'<strong>–û—à–∏–±–æ–∫:</strong> {analysis["error_count"]:,} ({analysis["error_rate"]:.1f}%)</p>'
            
            html += '<div class="metric-bar"><div class="metric-fill" style="width: ' + str(analysis['metrics']['precision']*100) + '%">Precision: ' + f'{analysis["metrics"]["precision"]:.3f}' + '</div></div>'
            html += '<div class="metric-bar"><div class="metric-fill" style="width: ' + str(analysis['metrics']['recall']*100) + '%">Recall: ' + f'{analysis["metrics"]["recall"]:.3f}' + '</div></div>'
            html += '<div class="metric-bar"><div class="metric-fill" style="width: ' + str(analysis['metrics']['f1']*100) + '%">F1: ' + f'{analysis["metrics"]["f1"]:.3f}' + '</div></div>'
            
            if analysis['most_confused_with']:
                html += '<p><strong>–û—Å–Ω–æ–≤–Ω—ã–µ –ø—É—Ç–∞–Ω–∏—Ü—ã:</strong></p><ul>'
                for conf in analysis['most_confused_with'][:3]:
                    html += f'<li><code>{conf["class"]}</code>: {conf["count"]:,} ({conf["percentage"]:.1f}%)</li>'
                html += '</ul>'
            
            html += f'<div class="interpretation-box"><strong>üìù –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:</strong><br>{analysis["interpretation"]}</div>'
            html += '</div>'
        
        return html
    
    def _generate_patterns_html(self, patterns):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –¥–ª—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—à–∏–±–æ–∫"""
        html = ''
        
        if patterns['concentrated_errors']:
            html += '<h3>üìå –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ (>60% –≤ –æ–¥–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏)</h3>'
            for ce in patterns['concentrated_errors']:
                html += f'<div class="pattern-card"><p><strong>{ce["class"]}</strong> ‚Üí <strong>{ce["main_confusion"]}</strong></p>'
                html += f'<p>–ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è: {ce["concentration"]:.1f}% | –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {ce["total_errors"]:,}</p>'
                html += '<p><em>–û—à–∏–±–∫–∏ —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–¥—É—Ç –≤ –æ–¥–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, —á—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ</em></p></div>'
        
        if patterns['scattered_errors']:
            html += '<h3>üåê –†–∞–∑–±—Ä–æ—Å–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ (<30% –≤ –ª—é–±–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏)</h3>'
            for se in patterns['scattered_errors']:
                html += f'<div class="pattern-card"><p><strong>{se["class"]}</strong></p>'
                html += f'<p>–û—à–∏–±–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É {se["error_distribution"]} –∫–ª–∞—Å—Å–∞–º–∏ | –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {se["total_errors"]:,}</p>'
                html += f'<p><em>{se["interpretation"]}</em></p></div>'
        
        if patterns['bidirectional_confusions']:
            html += '<h3>üîÑ –î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ –ø—É—Ç–∞–Ω–∏—Ü—ã</h3>'
            html += '<table><thead><tr><th>–ö–ª–∞—Å—Å 1</th><th>–ö–ª–∞—Å—Å 2</th><th>1‚Üí2</th><th>2‚Üí1</th><th>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</th></tr></thead><tbody>'
            for bc in patterns['bidirectional_confusions'][:10]:
                html += f'<tr><td>{bc["class1"]}</td><td>{bc["class2"]}</td>'
                html += f'<td>{bc["count1_to_2"]:,} ({bc["pct1_to_2"]:.1f}%)</td>'
                html += f'<td>{bc["count2_to_1"]:,} ({bc["pct2_to_1"]:.1f}%)</td>'
                html += f'<td>{bc["interpretation"]}</td></tr>'
            html += '</tbody></table>'
        
        return html
    
    def _generate_stability_html(self, stability):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤"""
        html = '<table><thead><tr><th>–ö–ª–∞—Å—Å</th><th>–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å</th><th>–ö–ª–∞—Å—Å–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏</th><th>–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è</th></tr></thead><tbody>'
        
        for s in stability:
            stability_color = '#4caf50' if s['stability_score'] > 0.7 else '#ff9800' if s['stability_score'] > 0.4 else '#f44336'
            html += f'<tr><td><strong>{s["class"]}</strong></td>'
            html += f'<td><span style="color: {stability_color}; font-weight: bold;">{s["stability_score"]:.3f}</span></td>'
            html += f'<td>{s["unique_error_classes"]}</td>'
            html += f'<td>{s["interpretation"]}</td></tr>'
        
        html += '</tbody></table>'
        return html
    
    def _generate_clusters_html(self, clusters):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø—É—Ç–∞–Ω–∏—Ü"""
        if not clusters:
            return '<p>–ö–ª–∞—Å—Ç–µ—Ä—ã –ø—É—Ç–∞–Ω–∏—Ü –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã.</p>'
        
        html = ''
        for i, cluster in enumerate(clusters[:15], 1):
            strength_color = {'—Å–∏–ª—å–Ω–∞—è': '#f44336', '—Å—Ä–µ–¥–Ω—è—è': '#ff9800', '—Å–ª–∞–±–∞—è': '#ffc107'}.get(cluster['strength'], '#666')
            html += f'<div class="cluster-item">'
            html += f'<h4>–ö–ª–∞—Å—Ç–µ—Ä {i}: {cluster["classes"][0]} ‚Üî {cluster["classes"][1]}</h4>'
            html += f'<p><strong>–°–∏–ª–∞ —Å–≤—è–∑–∏:</strong> <span style="color: {strength_color}; font-weight: bold;">{cluster["strength"]}</span></p>'
            html += f'<p>{cluster["classes"][0]} ‚Üí {cluster["classes"][1]}: {cluster["count1_to_2"]:,} ({cluster["pct1_to_2"]:.1f}%)</p>'
            html += f'<p>{cluster["classes"][1]} ‚Üí {cluster["classes"][0]}: {cluster["count2_to_1"]:,} ({cluster["pct2_to_1"]:.1f}%)</p>'
            html += f'<p><strong>–í—Å–µ–≥–æ –ø—É—Ç–∞–Ω–∏—Ü:</strong> {cluster["total_confusions"]:,}</p>'
            html += '</div>'
        
        return html
    
    def _generate_recommendations_html(self, recommendations):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        if not recommendations:
            return '<p>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.</p>'
        
        html = ''
        for i, rec in enumerate(recommendations, 1):
            priority_color = {'–∫—Ä–∏—Ç–∏—á–Ω—ã–π': '#f44336', '–≤—ã—Å–æ–∫–∏–π': '#ff9800', '—Å—Ä–µ–¥–Ω–∏–π': '#ffc107', '–Ω–∏–∑–∫–∏–π': '#4caf50'}.get(rec['priority'], '#666')
            html += f'<div class="recommendation">'
            html += f'<h4>{i}. [{rec["priority"].upper()}] {rec["type"].replace("_", " ").title()}</h4>'
            html += f'<p><strong>–û–ø–∏—Å–∞–Ω–∏–µ:</strong> {rec["description"]}</p>'
            html += f'<p><strong>–î–µ–π—Å—Ç–≤–∏–µ:</strong> {rec["action"]}</p>'
            html += '</div>'
        
        return html
    
    def _generate_categorization_html(self, categorization):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ—à–∏–±–æ–∫"""
        if not categorization or not categorization.get('category_summary'):
            return ''
        
        html = '<section id="categorization" class="section"><h2>üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –æ—à–∏–±–æ–∫</h2>'
        
        # –°–≤–æ–¥–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        html += '<h3>üìä –°–≤–æ–¥–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º</h3>'
        html += '<table><thead><tr><th>–ö–∞—Ç–µ–≥–æ—Ä–∏—è</th><th>–°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å</th><th>–ö–ª–∞—Å—Å–æ–≤</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th></tr></thead><tbody>'
        
        for cat_name, summary in categorization['category_summary'].items():
            severity_color = {
                '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è': '#f44336', '–≤—ã—Å–æ–∫–∞—è': '#ff9800',
                '—Å—Ä–µ–¥–Ω—è—è': '#ffc107', '–Ω–∏–∑–∫–∞—è': '#4caf50', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è': '#2196f3'
            }.get(summary['severity'], '#666')
            
            html += f'<tr>'
            html += f'<td><strong>{summary["name"].replace("_", " ").title()}</strong></td>'
            html += f'<td><span style="color: {severity_color}; font-weight: bold;">{summary["severity"]}</span></td>'
            html += f'<td>{summary["classes_count"]}</td>'
            html += f'<td>{summary["description"]}</td>'
            html += f'</tr>'
        
        html += '</tbody></table>'
        
        # –î–µ—Ç–∞–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        html += '<h3>üìã –î–µ—Ç–∞–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º</h3>'
        for cat_name, summary in categorization['category_summary'].items():
            if summary['classes_count'] > 0:
                severity_color = {
                    '–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è': '#f44336', '–≤—ã—Å–æ–∫–∞—è': '#ff9800',
                    '—Å—Ä–µ–¥–Ω—è—è': '#ffc107', '–Ω–∏–∑–∫–∞—è': '#4caf50', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è': '#2196f3'
                }.get(summary['severity'], '#666')
                
                html += f'<div class="pattern-card" style="border-left-color: {severity_color};">'
                html += f'<h4>{summary["name"].replace("_", " ").title()}</h4>'
                html += f'<p><strong>–°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å:</strong> <span style="color: {severity_color}; font-weight: bold;">{summary["severity"]}</span></p>'
                html += f'<p><strong>–û–ø–∏—Å–∞–Ω–∏–µ:</strong> {summary["description"]}</p>'
                html += f'<p><strong>–ö–ª–∞—Å—Å–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:</strong> {summary["classes_count"]}</p>'
                html += f'<p><strong>–ö–ª–∞—Å—Å—ã:</strong> {", ".join(summary["classes"])}</p>'
                
                if summary['recommendations']:
                    html += '<p><strong>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:</strong></p><ul>'
                    for rec in summary['recommendations']:
                        html += f'<li>{rec}</li>'
                    html += '</ul>'
                
                html += '</div>'
        
        html += '</section>'
        return html
    
    def _generate_visualizations_section(self, vis_paths):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–∏ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º–∏"""
        if not vis_paths:
            return ''
        
        html = '<section id="visualizations" class="section"><h2>üé® –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏</h2>'
        html += '<p>–ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ <code>confusion_matrix_figures/</code>:</p><ul>'
        for name, path in vis_paths.items():
            filename = os.path.basename(path)
            html += f'<li><strong>{name.replace("_", " ").title()}:</strong> <code>{filename}</code></li>'
        html += '</ul></section>'
        return html
    
    def plot_confusion_matrix(self, normalized=False, figsize=None, save_path=None, show=True):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è confusion matrix
        
        Args:
            normalized: –µ—Å–ª–∏ True, –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
            figsize: —Ä–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞)
            save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
            show: –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –≥—Ä–∞—Ñ–∏–∫
        """
        if not HAS_VISUALIZATION:
            raise ImportError("–î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å matplotlib –∏ seaborn: pip install matplotlib seaborn")
        
        if self.confusion_matrix is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å confusion matrix")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        if normalized:
            matrix_data = self.get_normalized_matrix()
            title = "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è Confusion Matrix (%)"
            fmt = '.1f'
            annot_kws = {'fontsize': 8}
        else:
            matrix_data = self.confusion_matrix
            title = "Confusion Matrix (–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)"
            fmt = 'd'
            annot_kws = {'fontsize': 9}
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∏–≥—É—Ä—ã
        n_classes = len(self.classes)
        if figsize is None:
            base_size = max(10, n_classes * 0.8)
            figsize = (base_size, base_size * 0.9)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–≥—É—Ä—ã
        fig, ax = plt.subplots(figsize=figsize)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ heatmap
        sns.heatmap(
            matrix_data,
            annot=True,
            fmt=fmt,
            cmap='Blues',
            cbar_kws={'label': '–ü—Ä–æ—Ü–µ–Ω—Ç' if normalized else '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'},
            square=True,
            linewidths=0.5,
            linecolor='gray',
            ax=ax,
            annot_kws=annot_kws,
            vmin=0
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ –º–µ—Ç–æ–∫
        ax.set_title(title, fontsize=14, fontweight='bold', pad=20)
        ax.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=12, fontweight='bold')
        ax.set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=12, fontweight='bold')
        
        # –ü–æ–≤–æ—Ä–æ—Ç –º–µ—Ç–æ–∫ –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ—á–Ω–æ—Å—Ç–∏
        diagonal_sum = sum(self.confusion_matrix.loc[cls, cls] for cls in self.classes)
        total = self.confusion_matrix.values.sum()
        accuracy = (diagonal_sum / total * 100) if total > 0 else 0
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é
        fig.text(0.5, 0.02, f'–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2f}% ({diagonal_sum:,}/{total:,})', 
                ha='center', fontsize=10, style='italic')
        
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
        
        if show:
            plt.show()
        else:
            plt.close()
        
        return fig
    
    def plot_metrics_comparison(self, save_path=None, show=True):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º
        
        Args:
            save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
            show: –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –≥—Ä–∞—Ñ–∏–∫
        """
        if not HAS_VISUALIZATION:
            raise ImportError("–î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å matplotlib –∏ seaborn: pip install matplotlib seaborn")
        
        if self.confusion_matrix is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å confusion matrix")
        
        metrics = self.calculate_metrics_from_matrix()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        classes_list = list(metrics.keys())
        precision = [metrics[cls]['precision'] for cls in classes_list]
        recall = [metrics[cls]['recall'] for cls in classes_list]
        f1 = [metrics[cls]['f1'] for cls in classes_list]
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–≥—É—Ä—ã
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º (—Å—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞)
        x = np.arange(len(classes_list))
        width = 0.25
        
        ax1.bar(x - width, precision, width, label='Precision', alpha=0.8)
        ax1.bar(x, recall, width, label='Recall', alpha=0.8)
        ax1.bar(x + width, f1, width, label='F1-Score', alpha=0.8)
        
        ax1.set_xlabel('–ö–ª–∞—Å—Å—ã', fontsize=12, fontweight='bold')
        ax1.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏', fontsize=12, fontweight='bold')
        ax1.set_title('–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º', fontsize=14, fontweight='bold')
        ax1.set_xticks(x)
        ax1.set_xticklabels(classes_list, rotation=45, ha='right')
        ax1.legend()
        ax1.grid(axis='y', alpha=0.3)
        ax1.set_ylim([0, 1.1])
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for i, (p, r, f) in enumerate(zip(precision, recall, f1)):
            ax1.text(i - width, p + 0.02, f'{p:.2f}', ha='center', va='bottom', fontsize=7)
            ax1.text(i, r + 0.02, f'{r:.2f}', ha='center', va='bottom', fontsize=7)
            ax1.text(i + width, f + 0.02, f'{f:.2f}', ha='center', va='bottom', fontsize=7)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: Heatmap –º–µ—Ç—Ä–∏–∫
        metrics_df = pd.DataFrame({
            'Precision': precision,
            'Recall': recall,
            'F1-Score': f1
        }, index=classes_list)
        
        sns.heatmap(
            metrics_df.T,
            annot=True,
            fmt='.3f',
            cmap='RdYlGn',
            vmin=0,
            vmax=1,
            cbar_kws={'label': '–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏'},
            ax=ax2,
            linewidths=0.5,
            linecolor='gray'
        )
        
        ax2.set_title('Heatmap –º–µ—Ç—Ä–∏–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º', fontsize=14, fontweight='bold')
        ax2.set_xlabel('–ö–ª–∞—Å—Å—ã', fontsize=12, fontweight='bold')
        ax2.set_ylabel('–ú–µ—Ç—Ä–∏–∫–∏', fontsize=12, fontweight='bold')
        plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
        
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
        
        if show:
            plt.show()
        else:
            plt.close()
        
        return fig
    
    def plot_error_analysis(self, top_n=15, save_path=None, show=True):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –æ—à–∏–±–æ–∫
        
        Args:
            top_n: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-–æ—à–∏–±–æ–∫ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
            show: –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –≥—Ä–∞—Ñ–∏–∫
        """
        if not HAS_VISUALIZATION:
            raise ImportError("–î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å matplotlib –∏ seaborn: pip install matplotlib seaborn")
        
        if self.confusion_matrix is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å confusion matrix")
        
        mistakes = self.find_common_mistakes(top_n=top_n)
        interpretation = self.interpret_errors()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–≥—É—Ä—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∞–º–∏
        fig = plt.figure(figsize=(16, 12))
        gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: –¢–æ–ø –æ—à–∏–±–æ–∫
        ax1 = fig.add_subplot(gs[0, :])
        if mistakes:
            true_classes = [f"{t} ‚Üí {p}" for t, p, _ in mistakes]
            counts = [c for _, _, c in mistakes]
            
            colors = plt.cm.Reds(np.linspace(0.4, 0.9, len(counts)))
            bars = ax1.barh(true_classes, counts, color=colors)
            ax1.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫', fontsize=11, fontweight='bold')
            ax1.set_title(f'–¢–æ–ø-{top_n} —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏', 
                         fontsize=12, fontweight='bold')
            ax1.grid(axis='x', alpha=0.3)
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
            for i, (bar, count) in enumerate(zip(bars, counts)):
                ax1.text(count + max(counts) * 0.01, i, f'{count:,}', 
                        va='center', fontsize=9)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–ª–∞—Å—Å—ã (error rate)
        ax2 = fig.add_subplot(gs[1, 0])
        if interpretation['problematic_classes']:
            problem_classes = [pc['class'] for pc in interpretation['problematic_classes']]
            error_rates = [pc['error_rate'] for pc in interpretation['problematic_classes']]
            
            colors = plt.cm.OrRd(np.linspace(0.5, 0.9, len(error_rates)))
            bars = ax2.barh(problem_classes, error_rates, color=colors)
            ax2.set_xlabel('–ü—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫ (%)', fontsize=11, fontweight='bold')
            ax2.set_title('–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–ª–∞—Å—Å—ã (>50% –æ—à–∏–±–æ–∫)', fontsize=12, fontweight='bold')
            ax2.grid(axis='x', alpha=0.3)
            ax2.set_xlim([0, 100])
            
            for bar, rate in zip(bars, error_rates):
                ax2.text(rate + 1, bar.get_y() + bar.get_height()/2, 
                        f'{rate:.1f}%', va='center', fontsize=9)
        else:
            ax2.text(0.5, 0.5, '–ù–µ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤', 
                    ha='center', va='center', transform=ax2.transAxes,
                    fontsize=12, style='italic')
            ax2.set_title('–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–ª–∞—Å—Å—ã', fontsize=12, fontweight='bold')
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –°–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏
        ax3 = fig.add_subplot(gs[1, 1])
        if interpretation['symmetric_errors']:
            symmetric_labels = [f"{se['class1']} ‚Üî {se['class2']}" 
                              for se in interpretation['symmetric_errors']]
            total_mistakes = [se['total_mistakes'] for se in interpretation['symmetric_errors']]
            
            colors = plt.cm.Purples(np.linspace(0.4, 0.9, len(total_mistakes)))
            bars = ax3.barh(symmetric_labels, total_mistakes, color=colors)
            ax3.set_xlabel('–í—Å–µ–≥–æ –æ—à–∏–±–æ–∫', fontsize=11, fontweight='bold')
            ax3.set_title('–°–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏', fontsize=12, fontweight='bold')
            ax3.grid(axis='x', alpha=0.3)
            
            for bar, count in zip(bars, total_mistakes):
                ax3.text(count + max(total_mistakes) * 0.01, bar.get_y() + bar.get_height()/2,
                        f'{count:,}', va='center', fontsize=9)
        else:
            ax3.text(0.5, 0.5, '–ù–µ—Ç —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã—Ö –æ—à–∏–±–æ–∫', 
                    ha='center', va='center', transform=ax3.transAxes,
                    fontsize=12, style='italic')
            ax3.set_title('–°–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏', fontsize=12, fontweight='bold')
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –ö–ª–∞—Å—Å—ã —Å –Ω–∏–∑–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é
        ax4 = fig.add_subplot(gs[2, :])
        if interpretation['low_performance_classes']:
            low_perf_classes = [lpc['class'] for lpc in interpretation['low_performance_classes']]
            low_precision = [lpc['precision'] for lpc in interpretation['low_performance_classes']]
            low_recall = [lpc['recall'] for lpc in interpretation['low_performance_classes']]
            low_f1 = [lpc['f1'] for lpc in interpretation['low_performance_classes']]
            
            x = np.arange(len(low_perf_classes))
            width = 0.25
            
            ax4.bar(x - width, low_precision, width, label='Precision', alpha=0.8)
            ax4.bar(x, low_recall, width, label='Recall', alpha=0.8)
            ax4.bar(x + width, low_f1, width, label='F1-Score', alpha=0.8)
            
            ax4.set_xlabel('–ö–ª–∞—Å—Å—ã', fontsize=11, fontweight='bold')
            ax4.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏', fontsize=11, fontweight='bold')
            ax4.set_title('–ö–ª–∞—Å—Å—ã —Å –Ω–∏–∑–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é (<50% –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º)', 
                         fontsize=12, fontweight='bold')
            ax4.set_xticks(x)
            ax4.set_xticklabels(low_perf_classes, rotation=45, ha='right')
            ax4.legend()
            ax4.grid(axis='y', alpha=0.3)
            ax4.set_ylim([0, 1])
            ax4.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='–ü–æ—Ä–æ–≥ 50%')
        else:
            ax4.text(0.5, 0.5, '–ù–µ—Ç –∫–ª–∞—Å—Å–æ–≤ —Å –Ω–∏–∑–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é', 
                    ha='center', va='center', transform=ax4.transAxes,
                    fontsize=12, style='italic')
            ax4.set_title('–ö–ª–∞—Å—Å—ã —Å –Ω–∏–∑–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é', fontsize=12, fontweight='bold')
        
        plt.suptitle('–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏', fontsize=16, fontweight='bold', y=0.995)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
        
        if show:
            plt.show()
        else:
            plt.close()
        
        return fig
    
    def create_all_visualizations(self, save_dir=None, show=False):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
        
        Args:
            save_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
            show: –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –≥—Ä–∞—Ñ–∏–∫–∏
        """
        if self.confusion_matrix is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å confusion matrix")
        
        if save_dir is None:
            save_dir = self.figures_dir
        
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. Confusion Matrix (–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
        path1 = os.path.join(save_dir, f'confusion_matrix_absolute_{timestamp}.png')
        self.plot_confusion_matrix(normalized=False, save_path=path1, show=show)
        
        # 2. Confusion Matrix (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è)
        path2 = os.path.join(save_dir, f'confusion_matrix_normalized_{timestamp}.png')
        self.plot_confusion_matrix(normalized=True, save_path=path2, show=show)
        
        # 3. –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
        path3 = os.path.join(save_dir, f'metrics_comparison_{timestamp}.png')
        self.plot_metrics_comparison(save_path=path3, show=show)
        
        # 4. –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
        path4 = os.path.join(save_dir, f'error_analysis_{timestamp}.png')
        self.plot_error_analysis(save_path=path4, show=show)
        
        print(f"\n‚úÖ –í—Å–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {save_dir}")
        
        return {
            'confusion_matrix_absolute': path1,
            'confusion_matrix_normalized': path2,
            'metrics_comparison': path3,
            'error_analysis': path4
        }


def load_predictions_from_file(predictions_file, true_labels_file=None):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞"""
    if predictions_file.endswith('.csv'):
        pred_df = pd.read_csv(predictions_file)
        if 'prediction' in pred_df.columns:
            y_pred = pred_df['prediction'].tolist()
        elif 'category' in pred_df.columns:
            y_pred = pred_df['category'].tolist()
        else:
            y_pred = pred_df.iloc[:, -1].tolist()
    elif predictions_file.endswith('.json'):
        with open(predictions_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, list):
                y_pred = [item.get('prediction', item.get('category', '')) for item in data]
            else:
                y_pred = data.get('predictions', [])
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {predictions_file}")
    
    if true_labels_file:
        if true_labels_file.endswith('.csv'):
            true_df = pd.read_csv(true_labels_file)
            if 'category' in true_df.columns:
                y_true = true_df['category'].tolist()
            else:
                y_true = true_df.iloc[:, -1].tolist()
        elif true_labels_file.endswith('.json'):
            with open(true_labels_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    y_true = [item.get('category', '') for item in data]
                else:
                    y_true = data.get('labels', [])
    else:
        df = pd.read_csv('dataset_balanced.csv')
        y_true = df['category'].tolist()
        y_true = y_true[:len(y_pred)]
    
    return y_true, y_pred


def create_demo_analysis():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ Confusion Matrix"""
    print("üéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó CONFUSION MATRIX")
    print("=" * 100)
    
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    df = pd.read_csv('dataset_balanced.csv')
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} –∑–∞–ø–∏—Å–µ–π")
    
    y_true = df['category'].tolist()
    classes = sorted(df['category'].unique())
    
    print(f"‚úÖ –ö–ª–∞—Å—Å–æ–≤: {len(classes)}")
    
    print("\nüîÆ –°–æ–∑–¥–∞–Ω–∏–µ baseline –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
    keywords_map = {
        '–ø–æ–ª—É—á–µ–Ω–∏–µ_–ø–æ—Å—ã–ª–∫–∏': ['–ø–æ–ª—É—á–∏—Ç—å', '–ø–æ—Å—ã–ª–∫–∞', '–∑–∞–∫–∞–∑', '–≤—ã–¥–∞—á–∞', '–∑–∞–±—Ä–∞—Ç—å'],
        '–ø—Ä–æ–±–ª–µ–º—ã_—Å_–∫–æ–¥–æ–º': ['–∫–æ–¥', '—Å–º—Å', '—Å–æ–æ–±—â–µ–Ω–∏–µ'],
        '—Å–≤—è–∑—å_—Å_–æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º': ['–æ–ø–µ—Ä–∞—Ç–æ—Ä', '—Å–≤—è–∑–∞—Ç—å—Å—è', '—Å–æ–µ–¥–∏–Ω–∏—Ç—å'],
        '—Å—Ç–∞—Ç—É—Å_–∑–∞–∫–∞–∑–∞': ['—Å—Ç–∞—Ç—É—Å', '–æ—Ç—Å–ª–µ–¥–∏—Ç—å', '–≥–¥–µ'],
        '–ø—Ä–æ–±–ª–µ–º—ã_–¥–æ—Å—Ç–∞–≤–∫–∏': ['–¥–æ—Å—Ç–∞–≤–∫–∞', '–∫—É—Ä—å–µ—Ä', '–∞–¥—Ä–µ—Å'],
        '–≤–æ–∑–≤—Ä–∞—Ç_–æ–±–º–µ–Ω': ['–≤–µ—Ä–Ω—É—Ç—å', '–æ–±–º–µ–Ω', '–∑–∞–º–µ–Ω–∞'],
        '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ_–ø—Ä–æ–±–ª–µ–º—ã': ['–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–æ—à–∏–±–∫–∞', '–ø—Ä–æ–±–ª–µ–º–∞'],
        '–∂–∞–ª–æ–±—ã': ['–∂–∞–ª–æ–±–∞', '–ø–ª–æ—Ö–æ', '–Ω–µ–¥–æ–≤–æ–ª–µ–Ω']
    }
    
    y_pred = []
    for text in df['text'].tolist():
        text_lower = str(text).lower()
        predicted = '–¥—Ä—É–≥–æ–µ'
        
        for category, keywords in keywords_map.items():
            if any(keyword in text_lower for keyword in keywords):
                predicted = category
                break
        
        y_pred.append(predicted)
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(y_pred)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    
    print("\nüìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ Confusion Matrix...")
    analyzer = ConfusionMatrixAnalyzer()
    matrix = analyzer.build_confusion_matrix(y_true, y_pred, classes)
    
    print(f"‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞: {len(classes)}x{len(classes)}")
    
    analyzer.print_detailed_report()
    analyzer.save_report('CONFUSION_MATRIX_REPORT.md', include_visualizations=True)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ HTML –æ—Ç—á–µ—Ç–∞ –¥–ª—è –≤–µ–±-–ø—Ä–æ—Å–º–æ—Ç—Ä–∞
    print("\nüåê –°–æ–∑–¥–∞–Ω–∏–µ HTML –æ—Ç—á–µ—Ç–∞ –¥–ª—è –≤–µ–±-–ø—Ä–æ—Å–º–æ—Ç—Ä–∞...")
    analyzer.save_html_report('CONFUSION_MATRIX_REPORT.html', include_visualizations=True)
    
    print("\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    return analyzer


def analyze_from_files(predictions_file, true_labels_file=None):
    """–ê–Ω–∞–ª–∏–∑ –∏–∑ —Ñ–∞–π–ª–æ–≤"""
    print("üéØ –ê–ù–ê–õ–ò–ó CONFUSION MATRIX –ò–ó –§–ê–ô–õ–û–í")
    print("=" * 100)
    
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    y_true, y_pred = load_predictions_from_file(predictions_file, true_labels_file)
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(y_pred)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(y_true)} –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫")
    
    classes = sorted(set(y_true) | set(y_pred))
    print(f"‚úÖ –ö–ª–∞—Å—Å–æ–≤: {len(classes)}")
    
    print("\nüìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ Confusion Matrix...")
    analyzer = ConfusionMatrixAnalyzer()
    matrix = analyzer.build_confusion_matrix(y_true, y_pred, classes)
    
    analyzer.print_detailed_report()
    analyzer.save_report('CONFUSION_MATRIX_REPORT.md', include_visualizations=True)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ HTML –æ—Ç—á–µ—Ç–∞ –¥–ª—è –≤–µ–±-–ø—Ä–æ—Å–º–æ—Ç—Ä–∞
    print("\nüåê –°–æ–∑–¥–∞–Ω–∏–µ HTML –æ—Ç—á–µ—Ç–∞ –¥–ª—è –≤–µ–±-–ø—Ä–æ—Å–º–æ—Ç—Ä–∞...")
    analyzer.save_html_report('CONFUSION_MATRIX_REPORT.html', include_visualizations=True)
    
    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    return analyzer


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import sys
    
    print("=" * 100)
    print("üìä CONFUSION MATRIX –ê–ù–ê–õ–ò–ó–ê–¢–û–† –î–õ–Ø DATA ANALYST")
    print("=" * 100)
    print(f"üìÖ –î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 100)
    
    if len(sys.argv) > 1:
        predictions_file = sys.argv[1]
        true_labels_file = sys.argv[2] if len(sys.argv) > 2 else None
        
        if not os.path.exists(predictions_file):
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {predictions_file}")
            return
        
        try:
            analyzer = analyze_from_files(predictions_file, true_labels_file)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
    else:
        try:
            analyzer = create_demo_analysis()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()

